<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Way of Numbers - Machine Learning</title><link href="https://wayofnumbers.github.io/" rel="alternate"></link><link href="https://wayofnumbers.github.io/feeds/machine-learning.atom.xml" rel="self"></link><id>https://wayofnumbers.github.io/</id><updated>2018-02-26T17:00:00-06:00</updated><subtitle>Data science for the rest of us.</subtitle><entry><title>Types of Optimization Algorithms used in Neural Networks and Ways to Optimize Gradient Descent</title><link href="https://wayofnumbers.github.io/Typtes-of-optimization-algorithms.html" rel="alternate"></link><published>2018-02-26T17:00:00-06:00</published><updated>2018-02-26T17:00:00-06:00</updated><author><name>Internet</name></author><id>tag:wayofnumbers.github.io,2018-02-26:/Typtes-of-optimization-algorithms.html</id><summary type="html">&lt;p&gt;Types of Optimization Algorithms used in Neural Networks and Ways to Optimize Gradient&amp;nbsp;Descent&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="https://towardsdatascience.com/types-of-optimization-algorithms-used-in-neural-networks-and-ways-to-optimize-gradient-95ae5d39529f"&gt;&lt;strong&gt;Original&amp;nbsp;Story&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://towardsdatascience.com/@anishsingh20"&gt;Anish Singh Walia&lt;/a&gt;:        &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;If your input data is sparse then methods such as &lt;strong&gt;&lt;span class="caps"&gt;SGD&lt;/span&gt;,&lt;span class="caps"&gt;NAG&lt;/span&gt; and momentum&lt;/strong&gt; are inferior and perform poorly. &lt;strong&gt;For sparse data sets one should use one of the adaptive learning-rate methods.&lt;/strong&gt; An additional benefit is that we won’t need to adjust the learning rate but likely achieve the best results with the default value.
If one wants fast convergence and train a deep Neural Network Model or a highly complex Neural Network then &lt;strong&gt;Adam or any other Adaptive learning rate techniques&lt;/strong&gt; should be used because they outperforms every other optimization&amp;nbsp;algorithms.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;One thing about Machine Learning the overal depth of the topics and algorithms makes it so easy to totally &lt;em&gt;&amp;#8216;sink&amp;#8217;&lt;/em&gt; yourself into it. And there is always something to dig. This article provides a view from a higher ground and compare different optimization algorithms and their application areas, thus pulling you out of the deep hole of deep&amp;nbsp;learning. &lt;/p&gt;
&lt;p&gt;A more visual example of these algorithms, see these two beautifully crafted&amp;nbsp;animations:&lt;/p&gt;
&lt;p&gt;&lt;img alt="SGD optimization on loss surface contours" src="https://wayofnumbers.github.io/images/optimization-algorithem-1.gif" title="SGD optimization on loss surface contours"&gt;
&lt;div style="text-align: center;"&gt;&lt;span class="caps"&gt;SGD&lt;/span&gt; optimization on loss surface contours&lt;/div&gt;
&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="SGD optimization on saddle point" src="https://wayofnumbers.github.io/images/optimization-algorithem-2.gif" title="SGD optimization on saddle point"&gt;
&lt;div style="text-align: center;"&gt;&lt;span class="caps"&gt;SGD&lt;/span&gt; optimization on saddle point&lt;/div&gt;&lt;/p&gt;</content><category term="machinelearning"></category><category term="AI"></category><category term="Optimization Algorithm"></category><category term="Gradient Descent"></category><category term="Neural Networks"></category></entry><entry><title>The AI Shortage</title><link href="https://wayofnumbers.github.io/The-AI-Shortage.html" rel="alternate"></link><published>2018-02-23T17:00:00-06:00</published><updated>2018-02-23T17:00:00-06:00</updated><author><name>Internet</name></author><id>tag:wayofnumbers.github.io,2018-02-23:/The-AI-Shortage.html</id><summary type="html">&lt;p&gt;&lt;span class="caps"&gt;AI&lt;/span&gt; talent shortage is not getting better any time&amp;nbsp;soon&amp;#8230;&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="https://medium.com/@Moscow25/the-ai-talent-shortage-704d8cf0c4cc"&gt;&lt;strong&gt;Original&amp;nbsp;Story&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://medium.com/@Moscow25"&gt;Nikolai Yakovenko&lt;/a&gt; from &lt;span class="caps"&gt;NVIDIA&lt;/span&gt;:        &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;But when I look for a designer, a Java developer, a real estate agent, etc — some are way better than others and deserve to get paid more than an &lt;span class="caps"&gt;AI&lt;/span&gt; researcher — but you’re fundamentally talking about pulling from a large well-balanced pool. It’s mostly an information game, and a matter of getting a little better than you need, but not much more than you can afford or should be paying.
In &lt;span class="caps"&gt;AI&lt;/span&gt;, it’s different. There just aren’t enough people to go around. And there aren’t enough people for every good project that can be attempted. Either academic, or something that if it works, can save the company&amp;nbsp;$1M.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The booming of a new disruptive technology always did this to the industry as well as the talent pool. It drives money into investing on the next big thing, and the money lures more talents into the field. There will be a shortage in the very beginning, and there will always be a surplus at the end of the curve. I&amp;#8217;m afraid &lt;span class="caps"&gt;AI&lt;/span&gt; won&amp;#8217;t be any different. It&amp;#8217;s just that the curve will take 10 maybe more years to unfold so it&amp;#8217;s not too late to get in the game if you think you have the stuff, since at the end of day, people with talent and grit will win, in every new technology &amp;#8216;gold&amp;nbsp;rush&amp;#8217;. &lt;/p&gt;</content><category term="machinelearning"></category><category term="AI"></category></entry></feed>