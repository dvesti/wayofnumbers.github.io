<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Way of Numbers</title><link href="https://wayofnumbers.github.io/" rel="alternate"></link><link href="https://wayofnumbers.github.io/feeds/all.atom.xml" rel="self"></link><id>https://wayofnumbers.github.io/</id><updated>2019-09-21T20:00:00-05:00</updated><subtitle>Data science for the rest of us.</subtitle><entry><title>How to Deploy Your Machine Learning Web App to Digital Ocean</title><link href="https://wayofnumbers.github.io/how-to-deploy-ML-web-app-to-DO.html" rel="alternate"></link><published>2019-09-21T20:00:00-05:00</published><updated>2019-09-21T20:00:00-05:00</updated><author><name>Michael Li</name></author><id>tag:wayofnumbers.github.io,2019-09-21:/how-to-deploy-ML-web-app-to-DO.html</id><summary type="html">&lt;p&gt;Guide to deploy your Machine Learning web app to&amp;nbsp;DigitalOcean&lt;/p&gt;</summary><content type="html">
&lt;p&gt;Using Fast.ai, Docker, GitHub, and Starlette &lt;span class="caps"&gt;ASGI&lt;/span&gt; Framework&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/5000/1*CIO_AZnYf8OkitOLyjXHHw.png"/&gt;&lt;/p&gt;
&lt;p&gt;You’ve collected your data, cleaned it up diligently, squeezed it into your carefully fine-tuned model and sweated many &lt;span class="caps"&gt;GPU&lt;/span&gt; hours and trained the model. The prediction is State-Of-The-Art! Bravo!&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;But what now?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Share it with the world of course! It has such great potential and no one has done this before, you want everyone to try it out! How? You ask.&lt;/p&gt;
&lt;p&gt;In this tutorial, I’ll introduce you to an affordable and flexible way of deploying your trained Machine Learning model. I’ll walk you through every step in the way and hopefully, after reading this article, you’ll have no issue deploying your ‘next big thing(model)’ to the world.&lt;/p&gt;
&lt;h2 id="how-to-trainand-export-your-dragonmodel"&gt;How to Train(and export) Your Dragon(Model)&lt;a class="headerlink" href="#how-to-trainand-export-your-dragonmodel" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img alt="Image from https://www.facebook.com/HowToTrainYourDragon/" src="https://cdn-images-1.medium.com/max/2000/0*Xd6meR5pp9AfdpXu"/&gt;&lt;em&gt;Image from &lt;a href="https://www.facebook.com/HowToTrainYourDragon/"&gt;https://www.facebook.com/HowToTrainYourDragon/&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;First of all, you need to train your model and export it. In this article, we’ll use Fast.ai’s library to showcase how it’s done. You may want to refer to my &lt;a href="https://medium.com/datadriveninvestor/deep-learning-models-by-fast-ai-library-c1cccc13e2b3"&gt;two-part articles&lt;/a&gt; about how to collect data and train a &lt;strong&gt;Chinese Calligraphy Classifier&lt;/strong&gt; model or you can also use your own model. For the purpose of this article, I’ll assume that you already trained the model and achieved your desired accuracy rate.&lt;/p&gt;
&lt;p&gt;Fast.ai uses a learn object to train the model, to export your model, use methodlearn.export() to export and save your trained model to a export.pkl file(&lt;em&gt;my model export file from the &lt;a href="https://medium.com/datadriveninvestor/deep-learning-models-by-fast-ai-library-c1cccc13e2b3"&gt;link&lt;/a&gt; above is around &lt;span class="caps"&gt;100MB&lt;/span&gt;&lt;/em&gt;). Save this file, we’ll use that later.&lt;/p&gt;
&lt;h2 id="github-driven-web-development"&gt;‘GitHub-Driven’ Web Development&lt;a class="headerlink" href="#github-driven-web-development" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;With the model ready, the next step is web app development. I assume you are a full-stack web developer, so let’s jump right into coding. No, I’m just kidding here. We’ll use a boilerplate web app template on GitHub to quickly get your web app ready. You only need to do some minor tweaks and you’ll be ready to go. If you don’t know what &lt;a href="https://github.com"&gt;GitHub&lt;/a&gt; is, it is a place to hold the source code of a lot of open-source applications. I already put a ready-made web app’s code there so you can easily download and reuse.&lt;/p&gt;
&lt;p&gt;Go to this&lt;a href="https://github.com/wayofnumbers/fastai-vision-uvicorn-gunicorn-starlette-docker"&gt; GitHub repository&lt;/a&gt;, click the big green button ‘&lt;strong&gt;Clone or download&lt;/strong&gt;’ on the right side, like below:&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/3534/1*jys25uR-Djp3uWpXCJEcOw.png"/&gt;&lt;/p&gt;
&lt;p&gt;On the pop-down window, copy the link, then go to your terminal and type:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;git&lt;/span&gt; &lt;span class="n"&gt;clone&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;https&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="o"&gt;//&lt;/span&gt;&lt;span class="n"&gt;github&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;com&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;wayofnumbers&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;fastai&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;vision&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;uvicorn&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;gunicorn&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;starlette&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;docker&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;git&lt;/span&gt;&lt;span class="p"&gt;](&lt;/span&gt;&lt;span class="n"&gt;https&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="o"&gt;//&lt;/span&gt;&lt;span class="n"&gt;github&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;com&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;wayofnumbers&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;fastai&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;vision&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;uvicorn&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;gunicorn&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;starlette&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;docker&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;git&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;cd&lt;/span&gt; &lt;span class="n"&gt;fastai&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;vision&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;uvicorn&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;gunicorn&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;starlette&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;docker&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;These commands will clone all the required code onto your local machine, under a folder named fastai-vision-uvicorn-gunicorn-starlette-docker and enter that folder. This is the main folder we’ll be working on, there are a couple of things in it that worth explaining:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;app&lt;/strong&gt;: The structure of this appfolder is as below:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;template&lt;/span&gt;
&lt;span class="o"&gt;|&lt;/span&gt;&lt;span class="c1"&gt;--app.html&lt;/span&gt;
&lt;span class="n"&gt;main&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;py&lt;/span&gt;
&lt;span class="n"&gt;export&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pkl&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This is where your Starlette web app source code resides. It has a very simple Python file main.py. The &lt;a href="https://www.starlette.io/"&gt;**Starlette &lt;/a&gt;**is a lightweight &lt;a href="https://asgi.readthedocs.io/en/latest/"&gt;&lt;span class="caps"&gt;ASGI&lt;/span&gt;&lt;/a&gt; framework/toolkit, which is ideal for building high-performance asyncio services.&lt;/p&gt;
&lt;p&gt;It also has the saved model fileexport.pkl. The template folder holds an &lt;span class="caps"&gt;HTML&lt;/span&gt; template file app.html which will serve as your web app &lt;span class="caps"&gt;UI&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Remember the exported export.pkl file you saved? Pull that out and replace the one in this app folder. So the app will use your model. You are also welcomed to update the app.html file for better-looking &lt;span class="caps"&gt;UI&lt;/span&gt;, but it’s not necessary as far as deployment is concerned. Now the source code of your web app is ready, we need to wrap it into a &lt;a href="https://www.docker.com/"&gt;Docker &lt;/a&gt;container and do some testing. We use the &lt;strong&gt;Dockerfile&lt;/strong&gt; as the config file. We’ll explore more in the next section.&lt;/p&gt;
&lt;h2 id="lets-dockerize-it"&gt;Let’s Dockerize it!&lt;a class="headerlink" href="#lets-dockerize-it" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;We will use Docker to create a container where our web app runs. If you don’t know what &lt;a href="https://www.docker.com/"&gt;Docker&lt;/a&gt; is, just know that it is kind of a mini virtual machine, with all the necessary libraries and dependencies installed so the app can run smoothly. It is smaller and more flexible than real Virtual Machine and can be created and deployed very easily.&lt;/p&gt;
&lt;p&gt;First, you need to install Docker. &lt;a href="https://docs.docker.com/install/"&gt;Here &lt;/a&gt;is a very thorough guide for your reference. After installation, if you are running Ubuntu, then it’s beneficial to run the following commands:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;sudo&lt;/span&gt; &lt;span class="n"&gt;groupadd&lt;/span&gt; &lt;span class="n"&gt;docker&lt;/span&gt;
&lt;span class="n"&gt;sudo&lt;/span&gt; &lt;span class="n"&gt;usermod&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;aG&lt;/span&gt; &lt;span class="n"&gt;docker&lt;/span&gt; &lt;span class="err"&gt;$&lt;/span&gt;&lt;span class="k"&gt;USER&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This will eliminate the need to use sudoevery time you enter a docker command. Reboot, now docker should be properly installed.&lt;/p&gt;
&lt;p&gt;In the same directory where app folder and Dockerfile resides, we need to create a docker image that contains all source code within this folder so we can test things out. Enter the following command(don’t forget the ‘.’ at the end):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;docker&lt;/span&gt; &lt;span class="n"&gt;build&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="n"&gt;test_app&lt;/span&gt; &lt;span class="p"&gt;.&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This will start a docker image building process according to the Dockerfile. It will take a while, so let’s take a brief look at what’s inside the Dockerfile:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;#&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="k"&gt;FROM&lt;/span&gt; &lt;span class="n"&gt;tiangolo&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;uvicorn&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;gunicorn&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;starlette&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;python3&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;7&lt;/span&gt;
&lt;span class="o"&gt;#&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="n"&gt;RUN&lt;/span&gt; &lt;span class="n"&gt;pip&lt;/span&gt; &lt;span class="n"&gt;install&lt;/span&gt; &lt;span class="n"&gt;fastai&lt;/span&gt; &lt;span class="n"&gt;aiohttp&lt;/span&gt; 
&lt;span class="o"&gt;#&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="n"&gt;RUN&lt;/span&gt; &lt;span class="n"&gt;pip&lt;/span&gt; &lt;span class="n"&gt;install&lt;/span&gt; &lt;span class="n"&gt;jinja2&lt;/span&gt; 
&lt;span class="o"&gt;#&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt; &lt;span class="n"&gt;RUN&lt;/span&gt; &lt;span class="n"&gt;pip&lt;/span&gt; &lt;span class="n"&gt;install&lt;/span&gt; &lt;span class="n"&gt;starlette&lt;/span&gt; 
&lt;span class="o"&gt;#&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt; &lt;span class="k"&gt;COPY&lt;/span&gt; &lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;app&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;app&lt;/span&gt; 
&lt;span class="o"&gt;#&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt; &lt;span class="n"&gt;WORKDIR&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;app&lt;/span&gt; 
&lt;span class="o"&gt;#&lt;/span&gt;&lt;span class="mi"&gt;7&lt;/span&gt; &lt;span class="n"&gt;EXPOSE&lt;/span&gt; &lt;span class="mi"&gt;80&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;It is quite self-explanatory:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Line 1:&lt;/strong&gt; Specify from which starter image we’ll build our docker image. We use tiangolo/uvicorn-gunicorn-starlette:python3.7 . You can find its GitHub link &lt;a href="https://github.com/tiangolo/uvicorn-gunicorn-starlette-docker"&gt;here&lt;/a&gt; and Docker Hub link &lt;a href="https://hub.docker.com/r/tiangolo/uvicorn-gunicorn-starlette/"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Line 2,3,4:&lt;/strong&gt; Install fast.ai library, jinja template framework, Starlette framework, and other utilities.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Line 5: &lt;/strong&gt;Copy your app folder into docker image so our app can run within the docker container.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Line 6, 7:&lt;/strong&gt; Assign work directory to the app folder and expose port 80 to outside so we can visit the web app through port 80(&lt;span class="caps"&gt;HTTP&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;Once the docker image is created, run docker images to check. You’ll find something like:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;REPOSITORY&lt;/span&gt;    &lt;span class="n"&gt;TAG&lt;/span&gt;    &lt;span class="n"&gt;IMAGE&lt;/span&gt; &lt;span class="n"&gt;ID&lt;/span&gt;    &lt;span class="n"&gt;CREATED&lt;/span&gt;        &lt;span class="k"&gt;SIZE&lt;/span&gt;
&lt;span class="n"&gt;test_app&lt;/span&gt;      &lt;span class="n"&gt;latest&lt;/span&gt; &lt;span class="n"&gt;xxxxxxxxx&lt;/span&gt;   &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="n"&gt;minutes&lt;/span&gt; &lt;span class="n"&gt;ago&lt;/span&gt;  &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;05&lt;/span&gt;&lt;span class="n"&gt;GB&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now we can fire up a docker container from the created image and test your app locally:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;docker&lt;/span&gt; &lt;span class="n"&gt;run&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="mi"&gt;80&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;80&lt;/span&gt; &lt;span class="err"&gt;\&lt;/span&gt;
        &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;v&lt;/span&gt; &lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="k"&gt;absolute&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="k"&gt;to&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;export&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pkl&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;app&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;export&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pkl&lt;/span&gt; &lt;span class="err"&gt;\&lt;/span&gt;
        &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;e&lt;/span&gt; &lt;span class="n"&gt;TITLE&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="ss"&gt;"Chinese Calligraphy Classifier"&lt;/span&gt; &lt;span class="err"&gt;\&lt;/span&gt;
        &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;e&lt;/span&gt; &lt;span class="n"&gt;SUBTITLE&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="ss"&gt;"Can disambiguate Chinese calligraphy styles like KaiShu, LiShu, XiaoZhuan"&lt;/span&gt; 
        &lt;span class="n"&gt;test_app&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;On the above docker command, we specified the port to be 80. We transferred two environment variables into the container, &lt;span class="caps"&gt;TITLE&lt;/span&gt; and &lt;span class="caps"&gt;SUBTITLE&lt;/span&gt;, they will be used to display our web app &lt;span class="caps"&gt;UI&lt;/span&gt; titles. At the end we specified our docker image name: test_app. Please note that for export.pkl file, you need to use the absolute path, otherwise Docker will not be able to find it.&lt;/p&gt;
&lt;p&gt;If you don’t see any error, your docker container should now be up and running. Head over to your browser and type 127.0.0.1 and hit enter, &lt;a href="https://context.reverso.net/translation/english-spanish/voil%C3%A0"&gt;voilà&lt;/a&gt;! You should see the web app. Give it a ‘Kaishu’, ‘Lishu’ or ‘Xiaozhuan’ calligraphy image and hit ‘&lt;strong&gt;Classify&lt;/strong&gt;’, you should see something like this:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Very rough web app UI" src="https://cdn-images-1.medium.com/max/2000/1*YvDNYLszPZgfC16I5NO2cg.png"/&gt;&lt;em&gt;Very rough web app &lt;span class="caps"&gt;UI&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;You can see the app classified this as ‘KaiShu’, which is correct. Now that your app is up and running on the local machine, we are 80% done. What’s left is to deploy it on the cloud. Let’s head to the cloud next!&lt;/p&gt;
&lt;h2 id="next-step-cloud"&gt;Next Step, Cloud!&lt;a class="headerlink" href="#next-step-cloud" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;For the cloud hosting service, we’ll use &lt;a href="https://www.digitalocean.com"&gt;DigitalOcean&lt;/a&gt;. Comparing to the more incumbent players like Amazon &lt;span class="caps"&gt;AWS&lt;/span&gt;, &lt;span class="caps"&gt;GCP&lt;/span&gt;, or Azure, it’s more friendly to developers and cheaper. You can follow &lt;a href="https://www.digitalocean.com/docs/droplets/how-to/create/"&gt;this well written and concise tutorial&lt;/a&gt; to create an account and a ‘Droplet’ of your own. (‘Droplet’ is a virtual machine running by Digital Ocean where you can install your app in, much like an &lt;span class="caps"&gt;AWS&lt;/span&gt; instance.) If you want, you can use &lt;a href="https://m.do.co/c/bc334d488542"&gt;this link&lt;/a&gt; to create your account and get $50 credit for free, which will be enough to get you started. Use the following configuration as a reference:&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/2428/1*lmMuFFvGN6jnxDRr0q2U1g.png"/&gt;&lt;/p&gt;
&lt;p&gt;It is recommended that you create your Droplets with at least &lt;strong&gt;4G&lt;/strong&gt; memory since installing PyTorch will require a lot of memory. You can resize it down to 2G later.&lt;/p&gt;
&lt;p&gt;You can choose the default ‘Data Center’ and set up your authentication method. Use &lt;span class="caps"&gt;SSH&lt;/span&gt; key or password, whichever way you feel more comfortable. I personally prefer &lt;span class="caps"&gt;SSH&lt;/span&gt; key, fewer keypresses and more secure. Once the Droplet is created, &lt;span class="caps"&gt;SSH&lt;/span&gt; into it and we are ready for the final deployment!&lt;/p&gt;
&lt;h2 id="deploy-deploy-deploy"&gt;Deploy! Deploy! Deploy！&lt;a class="headerlink" href="#deploy-deploy-deploy" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Now you should be able to &lt;span class="caps"&gt;SSH&lt;/span&gt; into your server as root. It’s recommended to create a normal user with sudo privilege, you can follow &lt;a href="https://www.digitalocean.com/community/tutorials/how-to-create-a-sudo-user-on-ubuntu-quickstart"&gt;this tutorial&lt;/a&gt;. Once a normal user is created, log out of your root user and &lt;span class="caps"&gt;SSH&lt;/span&gt; back into the server with your normal user account. The final deployment is very similar to what we’ve already done on our local machine, only this time we do it on the remote droplet server. First, let’s git clone our repo so we have the source code:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;git&lt;/span&gt; &lt;span class="n"&gt;clone&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;https&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="o"&gt;//&lt;/span&gt;&lt;span class="n"&gt;github&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;com&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;wayofnumbers&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;fastai&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;vision&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;uvicorn&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;gunicorn&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;starlette&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;docker&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;git&lt;/span&gt;&lt;span class="p"&gt;](&lt;/span&gt;&lt;span class="n"&gt;https&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="o"&gt;//&lt;/span&gt;&lt;span class="n"&gt;github&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;com&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;wayofnumbers&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;fastai&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;vision&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;uvicorn&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;gunicorn&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;starlette&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;docker&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;git&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;cd&lt;/span&gt; &lt;span class="n"&gt;fastai&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;vision&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;uvicorn&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;gunicorn&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;starlette&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;docker&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Don’t forget to copy your export.pkl file to replace what’s in the app folder. (follow this &lt;a href="https://unix.stackexchange.com/questions/106480/how-to-copy-files-from-one-machine-to-another-using-ssh"&gt;link&lt;/a&gt; if you don’t know how)&lt;/p&gt;
&lt;p&gt;If docker is not installed, install docker. Then build the docker image using below command. Again, if the image building failed due to low memory, resize your memory up, you can resize it down later without much cost increase.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;docker&lt;/span&gt; &lt;span class="n"&gt;build&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="n"&gt;test_app&lt;/span&gt; &lt;span class="p"&gt;.&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Once the image is built, fire up the docker container:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;docker&lt;/span&gt; &lt;span class="n"&gt;run&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="mi"&gt;80&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;80&lt;/span&gt; &lt;span class="err"&gt;\&lt;/span&gt;
        &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;v&lt;/span&gt; &lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="k"&gt;absolute&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="k"&gt;to&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;export&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pkl&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;app&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;export&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pkl&lt;/span&gt; &lt;span class="err"&gt;\&lt;/span&gt;
        &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;e&lt;/span&gt; &lt;span class="n"&gt;TITLE&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="ss"&gt;"Chinese Calligraphy Classifier"&lt;/span&gt; &lt;span class="err"&gt;\&lt;/span&gt;
        &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;e&lt;/span&gt; &lt;span class="n"&gt;SUBTITLE&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="ss"&gt;"Can disambiguate Chinese calligraphy styles like KaiShu, LiShu, XiaoZhuan"&lt;/span&gt; 
        &lt;span class="n"&gt;test_app&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Once the docker container is up and running, head over to your browser and enter your Droplet’s &lt;span class="caps"&gt;IP&lt;/span&gt; address, hit Enter. Congratulations! You’ve successfully deployed your Deep Learning model to the Internet!&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;a class="headerlink" href="#conclusion" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Not that hard, huh? Deployment using standard DigitalOcean Droplet offers a lot of flexibilities. You can do whatever you want to your Droplets since you have root access. You can run multiple apps on it, and pay very little($5 - $10 tier should be enough). If your app gets some traction and needs more resources, you can easily scale up.&lt;/p&gt;
&lt;p&gt;I hope this tutorial somewhat help you deploy your &lt;span class="caps"&gt;AI&lt;/span&gt; app. If you have any question or want to share your deployment experience, please write a response below. Happy Deploying!&lt;/p&gt;</content><category term="Machine Learning"></category><category term="AI"></category><category term="Deep Learning"></category><category term="fast.ai"></category><category term="DigitalOcean"></category><category term="Docker"></category><category term="Starlette"></category></entry><entry><title>OpenAI: Catch Me If You Can</title><link href="https://wayofnumbers.github.io/openai-catch-me-if-you-can.html" rel="alternate"></link><published>2019-09-19T20:00:00-05:00</published><updated>2019-09-19T20:00:00-05:00</updated><author><name>Michael Li</name></author><id>tag:wayofnumbers.github.io,2019-09-19:/openai-catch-me-if-you-can.html</id><summary type="html">&lt;p&gt;OpenAI&amp;#8217;s multi-agent hide and seek experiments&amp;nbsp;explained.&lt;/p&gt;</summary><content type="html">
&lt;p&gt;What OpenAI’s Multi-Agent Hide and Seek Break Through Means&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/5074/1*zx1DVdwYOVJWdHURXr5qjw.png"/&gt;&lt;/p&gt;
&lt;h3 id="who-is-openai"&gt;Who is OpenAI?&lt;a class="headerlink" href="#who-is-openai" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;When it comes to reinforcement learning, OpenAI is a big name. The &lt;a href="https://gym.openai.com/"&gt;OpenAI Gym toolkit&lt;/a&gt; provides a solid foundation for a lot of &lt;span class="caps"&gt;ML&lt;/span&gt; researchers to explore and study reinforcement learning techniques. They also are known to have developed ‘&lt;a href="https://openai.com/blog/better-language-models/"&gt;&lt;span class="caps"&gt;GPT&lt;/span&gt;-2&lt;/a&gt;’ language model. The ‘deep fake’ news the model generated is so scarily good that OpenAI refused to release the trained model, just the &lt;a href="https://github.com/openai/gpt-2"&gt;code &lt;/a&gt;and &lt;a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf"&gt;paper&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img alt="From OpenAI.com" src="https://cdn-images-1.medium.com/max/2406/1*p3ZS8EKITYRCTu1_JqtwiA.png"/&gt;&lt;em&gt;From OpenAI.com&lt;/em&gt;&lt;/p&gt;
&lt;h3 id="hide-and-seek-only-this-time-the-computer-is-playing-it"&gt;Hide and Seek, Only This Time the Computer is Playing It&lt;a class="headerlink" href="#hide-and-seek-only-this-time-the-computer-is-playing-it" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Today they release something new and equally groundbreaking: &lt;a href="https://openai.com/blog/emergent-tool-use/"&gt;The Multi-agent Hide and Seek&lt;/a&gt;. You can have a look at the short video below from their website:&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;iframe allowfullscreen="" frameborder="0" height="315" src="https://www.youtube.com/embed/kopoLzvh5jY" width="560"&gt;&lt;/iframe&gt;&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;Simply put, they designed a reinforcement agent and let it play a simple ‘Hide and Seek’ game that we all played when we were still a kid. After millions of games played with itself and the prior version of itself. Both the seeking and hiding agent developed several very effective strategies to counter each other and win.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Basic abilities agent has to play and win the game" src="https://cdn-images-1.medium.com/max/2000/1*ldh0SVwH9-Z8iZWY_MGhTw.gif"/&gt;&lt;em&gt;Basic abilities agent has to play and win the game&lt;/em&gt;&lt;/p&gt;
&lt;h3 id="how-to-train-your-dragon-urr-agent"&gt;How to Train Your Dragon, urr… Agent&lt;a class="headerlink" href="#how-to-train-your-dragon-urr-agent" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;The agents develop their tactics in an adversarial way.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Both agents move randomly&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Seeking agent learns to chase hiding agent.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Hiding agent learns to run away from seeking agent (not working)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Hiding agent learns to use blocks in the environment to block entrance to a small room and hide inside (It works!)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Seeking agent learns to use ramp blocks in the environment to jump over the wall to get inside the blocked room&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Hiding agent learns to move ramp blocks inside the room, then block the room&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;…… the game keeps playing and more gaming mechanism is discovered and exploited, both agents getting smarter and smarter.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The seeking agent and hiding agent counter each other and at the same time grow with each other. This is a concept (相生相克 mutually reinforce and neutralize each other) very familiar to Chinese people, and can be illustrated in a simple way:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Taichi 太极" src="https://cdn-images-1.medium.com/max/2000/1*w3rxOO7r5d96JdDDAYskpQ.gif"/&gt;&lt;em&gt;Taichi 太极&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Philosophy aside, we did see this kind of concept appear on recent &lt;span class="caps"&gt;AI&lt;/span&gt; field quite often. Another example is the rise of &lt;a href="https://towardsdatascience.com/must-read-papers-on-gans-b665bbae3317"&gt;&lt;span class="caps"&gt;GAN&lt;/span&gt; (General Adversarial Network)&lt;/a&gt; where a ‘generator’ and ‘discriminator’ are trained at the same time to achieve state-of-the-art results.&lt;/p&gt;
&lt;h3 id="the-agents-looks-cute-but-why-i-still-feels-a-bit-chilly-on-my-spine"&gt;The Agents Looks Cute! But Why I Still Feels a Bit Chilly on My Spine?&lt;a class="headerlink" href="#the-agents-looks-cute-but-why-i-still-feels-a-bit-chilly-on-my-spine" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Agents in the game are quite cute with cartoony big heads and smiley eyes. But underneath the cuteness, what does the great results suggest? Well, just imagine, if they are not playing this cute little hide and seek game where agents giggles when get caught, rather, they are playing Doom or Quake, where blood and gores fly around when the agent gets caught. Will the bloody scene lead you to start worrying about the possible application of this model and the potential it has if weaponized? If this still seems too far away from reality, let me bring this uncomfortable imagination one step further, allow me to use three words:&lt;/p&gt;
&lt;blockquote&gt;
&lt;h1 id="boston-dynamic-drones-skynet"&gt;Boston Dynamic, Drones, Skynet.&lt;a class="headerlink" href="#boston-dynamic-drones-skynet" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h1&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;center&gt;&lt;iframe allowfullscreen="" frameborder="0" height="315" src="https://www.youtube.com/embed/dKjCWfuvYxQ" width="560"&gt;&lt;/iframe&gt;&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;The tasks and tactics agent learned from millions of games might still seem easy. Hide, use blocks, use ramps, etc. But don’t forget that complicated and sophisticated strategy is formed with all these small pieces. One big advancement of &lt;span class="caps"&gt;AI&lt;/span&gt; recently is &lt;a href="https://en.wikipedia.org/wiki/Transfer_learning"&gt;transfer learning&lt;/a&gt;, build new &lt;span class="caps"&gt;AI&lt;/span&gt; models on top of already trained/learned models. (Using transfer learning based on already trained &lt;a href="http://www.image-net.org/"&gt;&lt;span class="caps"&gt;IMAGENET&lt;/span&gt;&lt;/a&gt; model, people can quickly train a fine-grained cat/dog classifier with only 100 images and 1 &lt;span class="caps"&gt;GPU&lt;/span&gt; in minutes. I explained the approach of &lt;a href="https://course.fast.ai/videos/?lesson=1"&gt;fast.ai&lt;/a&gt; at &lt;a href="https://medium.com/datadriveninvestor/deep-learning-models-by-fast-ai-library-c1cccc13e2b3"&gt;here&lt;/a&gt;). These basic game tactic model can be utilized in the future to build more realistic and dangerous military strategy models that can totally be applied in war.&lt;strong&gt; This is not beyond our reach now. &lt;/strong&gt;If we put all our current &lt;span class="caps"&gt;AI&lt;/span&gt; and robotic achievements together, great/scary things can be achieved.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;When an OpenAI model agent running within a Boston Dynamics robot or killer drones, and video surveillance networks everywhere to watch your every step, if you are the hider playing this game, what is the chance of you winning?&lt;/p&gt;
&lt;/blockquote&gt;</content><category term="Machine Learning"></category><category term="AI"></category><category term="OpenAI"></category><category term="Gaming"></category><category term="Reinforcement Learning"></category></entry><entry><title>How I Trained Computer to Learn Calligraphy Styles: Part 2</title><link href="https://wayofnumbers.github.io/chinese-calligraphy-classifier-2.html" rel="alternate"></link><published>2019-09-16T20:00:00-05:00</published><updated>2019-09-16T20:00:00-05:00</updated><author><name>Michael Li</name></author><id>tag:wayofnumbers.github.io,2019-09-16:/chinese-calligraphy-classifier-2.html</id><summary type="html">&lt;p&gt;Fine-tune model for Chinese Calligraphy Classifier with fast.ai&amp;nbsp;library&lt;/p&gt;</summary><content type="html">
&lt;p&gt;Build a Deep Learning Model with fast.ai Library&lt;/p&gt;
&lt;p&gt;&lt;img alt="Photo by Kon Karampelas on Unsplash" src="https://cdn-images-1.medium.com/max/12000/0*gzpUfcpouuU10xO1"/&gt;&lt;em&gt;Photo by &lt;a href="https://unsplash.com/@konkarampelas?utm_source=medium&amp;amp;utm_medium=referral"&gt;Kon Karampelas&lt;/a&gt; on &lt;a href="https://unsplash.com?utm_source=medium&amp;amp;utm_medium=referral"&gt;Unsplash&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;I wanted to start a series of posts for the projects I finished/polished for my &lt;a href="https://course.fast.ai/"&gt;Practical Deep Learning for Coders&lt;/a&gt; fast.ai course. Since I’m pretty green on &lt;span class="caps"&gt;ML&lt;/span&gt;/&lt;span class="caps"&gt;DL&lt;/span&gt; field, I hope the challenges I faced and overcome could be of value for other people experiencing the same journey.&lt;/p&gt;
&lt;p&gt;Model &lt;a href="https://medium.com/@lymenlee/deep-learning-models-by-fast-ai-library-c1cccc13e2b3"&gt;1&lt;/a&gt; ・&lt;a href="https://medium.com/datadriveninvestor/chinese-calligraphy-classifier-fine-tuning-cbfbf0e304d8"&gt;1a&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="making-it-even-better"&gt;Making It Even Better&lt;a class="headerlink" href="#making-it-even-better" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;In my &lt;a href="https://medium.com/datadriveninvestor/deep-learning-models-by-fast-ai-library-c1cccc13e2b3"&gt;last post&lt;/a&gt;, I explained the approach I take for this image recognition problem using fast.ai library. As you can see, once we get the data down to a fast.ai ImageDataBunch, the code is rather simple and we achieve a 90% accuracy rate, which is quite impressive considering the quality of our data(randomly downloaded from Google/Baidu search without much data cleaning). Now, can we do better?
&lt;a href="https://www.datadriveninvestor.com/2019/03/03/editors-pick-5-machine-learning-books/"&gt;&lt;strong&gt;&lt;span class="caps"&gt;DDI&lt;/span&gt; Editor’s Pick: 5 Machine Learning Books That Turn You from Novice to Expert | Data Driven…&lt;/strong&gt;
&lt;em&gt;The booming growth in the Machine Learning industry has brought renewed interest in people about Artificial…&lt;/em&gt;www.datadriveninvestor.com&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;h1 id="turns-out-we-can"&gt;Turns out, we can!&lt;a class="headerlink" href="#turns-out-we-can" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h1&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/2000/1*sz46EDt2HU_N2YecCXJ9ng.jpeg"/&gt;&lt;/p&gt;
&lt;p&gt;How? Well, there are two things in our prior pipeline that could improve:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Image Pre-processing Tweak&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Model Training Fine Tune.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Let’s dive deeper.&lt;/p&gt;
&lt;h3 id="image-pre-processing-tweak"&gt;Image Pre-Processing Tweak&lt;a class="headerlink" href="#image-pre-processing-tweak" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Remember when we import our data into fast.ai ImageDataBunch, we used the following code:&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/2000/0*Prt5fbhL-qj1OoQE.png"/&gt;&lt;/p&gt;
&lt;p&gt;Notice that on our image pre-processing, i.e. get_transforms function, we didn’t give it any parameter and just used the default. The default will try to apply a variety of image augmentation techniques to make the image data-set generalize better, like flipping, warping, rotating, cropping, etc. This is good, fast.ai library helped us do the ‘best practice’ for the majority of the cases. But in our case here, some default might not work that well.&lt;/p&gt;
&lt;p&gt;The biggest one is ‘flipping’. Because we are trying to classify calligraphy artworks and in real life, it will never randomly flip left/right or up/down. So making the images flips randomly will not reflect the real-life cases and thus won’t help with our training accuracy.&lt;/p&gt;
&lt;p&gt;To fix this, we tweaked our code as below:&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/2000/1*9Gv0vDlF12MPKznehyU1LA.png"/&gt;&lt;/p&gt;
&lt;p&gt;Notice we pass do_flip=False into the get_transforms function, thus telling the module to not randomly flipping our images during importing.&lt;/p&gt;
&lt;h3 id="model-training-fine-tune"&gt;Model Training Fine Tune&lt;a class="headerlink" href="#model-training-fine-tune" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Now that the image pre-processing is done. We can re-structure out model training to avoid overfitting and achieve better accuracy. This approach is introduced in the fast.ai &lt;a href="https://course.fast.ai/"&gt;Practical Deep Learning for Coders&lt;/a&gt; course &lt;a href="https://course.fast.ai/videos/?lesson=3"&gt;lesson 3&lt;/a&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Instead of training the model directly from a 256x256 image size, we’ll gradually scaling up the image size. More concretely, we will first train a &lt;span class="caps"&gt;CNN&lt;/span&gt; to classify the images of 128x128 size, once we achieved best accuracy, we’ll then use transfer learning and keep training the model on the same data-set, except with 256x256 image size. We’ll call the 128x128 image size training ‘stage 1’ and 256x256 image size training ‘stage 2’&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;After our stage 1 training(where my &lt;a href="https://medium.com/datadriveninvestor/deep-learning-models-by-fast-ai-library-c1cccc13e2b3"&gt;last post&lt;/a&gt; left off), we have a trained &lt;span class="caps"&gt;CNN&lt;/span&gt; model called learn , it’s ‘unfreezed’ and achieves an accuracy of around 85%.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Accuracy 86% after training a 128x128 image size CNN." src="https://cdn-images-1.medium.com/max/2000/1*gereMOAvFIDiK2Mposxw4g.png"/&gt;&lt;em&gt;Accuracy 86% after training a 128x128 image size &lt;span class="caps"&gt;CNN&lt;/span&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Now we need to freeze the network again, create a new ImageDataBunch with 256x256 image size and restart the same training process.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/2000/1*uhQ8i6QTzLJQ1J9EwNtcsg.png"/&gt;&lt;/p&gt;
&lt;p&gt;After finding the best learning rate, we train the &lt;span class="caps"&gt;CNN&lt;/span&gt; with another 2 epochs, already breaking into 91% accuracy. We’ll then do the same ‘unfreeze’ and keep training.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/2000/1*CAwRb2bFZFpgTe8u5UA9JQ.png"/&gt;&lt;/p&gt;
&lt;p&gt;After unfreeze, we trained the model with another 4 epochs, the accuracy broke into &lt;strong&gt;96.5%&lt;/strong&gt;. Observed that valudation_losshas already surpassed training_loss, suggesting a sign of overfitting. We’ll stop our training here.&lt;/p&gt;
&lt;p&gt;This simple technique is also called ‘&lt;strong&gt;Progressive resizing&lt;/strong&gt;’ by &lt;a href="undefined"&gt;Jeremy Howard&lt;/a&gt; from &lt;a href="https://www.fast.ai/2018/08/10/fastai-diu-imagenet/"&gt;fast.ai&lt;/a&gt; and helped his team &lt;a href="https://www.theverge.com/2018/5/7/17316010/fast-ai-speed-test-stanford-dawnbench-google-intel"&gt;beat Google in a competition of speed training &lt;span class="caps"&gt;IMAGENET&lt;/span&gt; in *DAWNBench&lt;/a&gt; by training the &lt;span class="caps"&gt;IMAGGNET&lt;/span&gt; in a whopping&lt;strong&gt;18&lt;/strong&gt; minutes and &lt;strong&gt;\$40&lt;/strong&gt; Amazon &lt;span class="caps"&gt;AWS&lt;/span&gt; cost.*&lt;/p&gt;
&lt;h3 id="to-wrap-it-up"&gt;To Wrap It Up&lt;a class="headerlink" href="#to-wrap-it-up" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;&lt;img alt="Photo by Franki Chamaki on Unsplash" src="https://cdn-images-1.medium.com/max/8064/0*ccqj05oUPQjsG_Jk"/&gt;&lt;em&gt;Photo by &lt;a href="https://unsplash.com/@franki?utm_source=medium&amp;amp;utm_medium=referral"&gt;Franki Chamaki&lt;/a&gt; on &lt;a href="https://unsplash.com?utm_source=medium&amp;amp;utm_medium=referral"&gt;Unsplash&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;With two simple tweaks, we managed to increase the accuracy around 6.5%, breaking into the state-of-the-art range of results. Major takeaways:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;When doing image pre-processing, make sure the processed images still properly represent what real-life data will look like.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The reason gradually increase training image size works is: by giving the trained model a data-set that’s 4 times bigger, actually means giving the model a brand new data to train, avoiding overfitting.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Starting from smaller sized images for training will also have the benefit of faster training and quicker experimenting. This usually leads to better results.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;That’s it for Chinese Calligraphy Classifier. I hope you learned a thing or two after reading these two articles. We’re trying to get some specific calligrapher’s ‘true’ and ‘fake’ artworks and see if we can build a ‘true or false’ classifier. This will be a very interesting and much valuable next step. Will report back and write more articles if we made real progress. But until then, we’ll move on to put this well-trained model into production and build a web-app around it. Stay tuned.&lt;/p&gt;
&lt;p&gt;If you haven’t read my first post on this topic, here’s the link:
&lt;a href="https://medium.com/datadriveninvestor/deep-learning-models-by-fast-ai-library-c1cccc13e2b3"&gt;&lt;strong&gt;How I Trained Computer to Learn Calligraphy Styles: Part1&lt;/strong&gt;
&lt;em&gt;Build a Deep Learning Model with fast.ai Library&lt;/em&gt;medium.com&lt;/a&gt;&lt;/p&gt;</content><category term="Machine Learning"></category><category term="AI"></category><category term="Deep Learning"></category><category term="fast.ai"></category><category term="calligraphy"></category></entry><entry><title>How I Trained Computer to Learn Calligraphy Styles: Part 1</title><link href="https://wayofnumbers.github.io/chinese-calligraphy-classifier-1.html" rel="alternate"></link><published>2019-09-15T20:00:00-05:00</published><updated>2019-09-15T20:00:00-05:00</updated><author><name>Michael Li</name></author><id>tag:wayofnumbers.github.io,2019-09-15:/chinese-calligraphy-classifier-1.html</id><summary type="html">&lt;p&gt;Build a model for Chinese Calligraphy Classifier with fast.ai&amp;nbsp;library&lt;/p&gt;</summary><content type="html">
&lt;p&gt;Build a Deep Learning Model with fast.ai Library&lt;/p&gt;
&lt;p&gt;&lt;img alt="Photo by Raychan on Unsplash" src="https://cdn-images-1.medium.com/max/10944/0*1vRfrkhsQiTkkBgJ"/&gt;&lt;em&gt;Photo by &lt;a href="https://unsplash.com/@wx1993?utm_source=medium&amp;amp;utm_medium=referral"&gt;Raychan&lt;/a&gt; on &lt;a href="https://unsplash.com?utm_source=medium&amp;amp;utm_medium=referral"&gt;Unsplash&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;I wanted to start a series of posts for the projects I finished/polished for my &lt;a href="https://course.fast.ai/"&gt;Practical Deep Learning for Coders&lt;/a&gt; fast.ai course. Since I’m pretty green on &lt;span class="caps"&gt;ML&lt;/span&gt;/&lt;span class="caps"&gt;DL&lt;/span&gt; field, I hope the challenges I faced and overcome could be of value for other people experiencing the same journey.&lt;/p&gt;
&lt;p&gt;Model &lt;a href="https://medium.com/@lymenlee/deep-learning-models-by-fast-ai-library-c1cccc13e2b3"&gt;1&lt;/a&gt; ・&lt;a href="https://medium.com/datadriveninvestor/chinese-calligraphy-classifier-fine-tuning-cbfbf0e304d8"&gt;1a&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="why-build-a-chinese-calligraphy-classifier"&gt;&lt;strong&gt;Why Build a Chinese Calligraphy Classifier&lt;/strong&gt;&lt;a class="headerlink" href="#why-build-a-chinese-calligraphy-classifier" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Like any calligraphy, Chinese calligraphy is a form of art. Some great pieces written by some ancient masters have both great art value and economic values (selling at multi-million dollars on auctions).&lt;/p&gt;
&lt;p&gt;&lt;img alt="*Jieshi Tie* by Song Dynasty politician and scholar Zeng Gong, $30,000,000" src="https://cdn-images-1.medium.com/max/2000/1*2lrTyRMYIcm6HfnojdgUvg.jpeg"/&gt;*&lt;em&gt;Jieshi Tie&lt;/em&gt; by Song Dynasty politician and scholar Zeng Gong, \$30,000,000*&lt;/p&gt;
&lt;p&gt;There are multiple main styles/schools of calligraphy, mainly belongs to different dynasties. Each has its own way of shaping the character and arranging them. The differences are subtle and abstract. It makes sense to see if a trained deep learning model can do a good job of telling which style it is.
&lt;a href="https://www.datadriveninvestor.com/2019/03/03/editors-pick-5-machine-learning-books/"&gt;&lt;strong&gt;&lt;span class="caps"&gt;DDI&lt;/span&gt; Editor’s Pick: 5 Machine Learning Books That Turn You from Novice to Expert | Data Driven…&lt;/strong&gt;
&lt;em&gt;The booming growth in the Machine Learning industry has brought renewed interest in people about Artificial…&lt;/em&gt;www.datadriveninvestor.com&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;I picked three styles:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Lishu(隶书)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Kaishu(楷书)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Xiaozhuan(小篆)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;as a proof-of-concept. Once successful trained, the model could serve as a transfer learning base-model for the more fine-grained classifier( e.g. calligraphers classifier). This has some real-life value. From time to time, some ancient artifacts are discovered and some of them are calligraphy artworks. Sometimes it’s hard to tell whose work it is. Is it valuable (like undiscovered artwork by a famous calligrapher)?&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;This calligrapher classifier can serve as a way to quickly identify artworks by great calligraphers. ( Finding diamond in the rough &lt;em&gt;😉&lt;/em&gt;)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id="collecting-data"&gt;Collecting Data&lt;a class="headerlink" href="#collecting-data" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;To build a calligraphy classifier, we will need some calligraphy examples of each style. I did some search online and cannot find any good already-made data-set for different calligraphy styles. So I’ll have to build it myself.&lt;/p&gt;
&lt;p&gt;Building a images data-set isn’t hard thanks to Google’s Images search functionality and some JavaScript snippets. Here’s how:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Go to &lt;a href="https://www.google.com/imghp?hl=en"&gt;Google Images&lt;/a&gt; and search for “隶书 字帖 网格” (lishu, characters book, grid), this will give you the most relevant results.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Scroll down to show more results, you’ll hit the bottom with ‘&lt;em&gt;Show more results&lt;/em&gt;’ button. Click if you want more, but keep in mind that &lt;strong&gt;700&lt;/strong&gt; images is the maximum here.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img alt="Google search results for Lishu style" src="https://cdn-images-1.medium.com/max/2000/1*uQPNDb-qXO3mYQIHuxitMQ.png"/&gt;&lt;em&gt;Google search results for Lishu style&lt;/em&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Now is where the magic happens. Press Ctrl+Shift+J in Windows/Linux and Cmd+Opt+J in Mac to bring up the JavaScript ‘Console’ window of the browser. The following JavaScript snippet will get the URLs of each of the images.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;4) If successfully run, a text file will be downloaded with all the URLs for the images in your search results. You can then set up a folder and use fast.ai’s ‘download_images’ function to download these images.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/2000/1*19mOhygnBZfGmX4S2fD4ww.png"/&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Rinse and repeat for other styles. You might want to put them into different folders like kaishu, xiaozhuan and put them all under a folder called train so later on, fast.ai can easily import them into the model.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Alternatively, you can also go to Baidu.com for images search, using this &lt;a href="https://gist.github.com/wayofnumbers/39842bb909c04070de49e53c418d512f"&gt;snippet&lt;/a&gt; to automatically download the images you searched for. (Code too long to be put into this post, so I link it here).&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id="lets-have-a-look-at-the-data"&gt;Let’s Have a Look at the Data&lt;a class="headerlink" href="#lets-have-a-look-at-the-data" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;If you organize the downloaded images into train/lishu, train/kaishu, train/xiaozhuan, then you can run the following code to import them into and transformed accordingly, ready to fit a model.fast.ai’s powerfulImageDataBunch object, where all data is organized, splitted and transformed accordingly, ready to fit a model.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/2000/1*hsF08GhTX9tjyov8hHzbFw.png"/&gt;&lt;/p&gt;
&lt;p&gt;Note that we split the train/validation set with an 80/20 ratio, image resized to 224 which is a good default for any image recognition task.&lt;/p&gt;
&lt;p&gt;Now that data is imported properly, let’s look at our images:&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/2000/1*F378nVvqW7o6lz6QxAB2gA.png"/&gt;&lt;/p&gt;
&lt;p&gt;As we can see from the few examples above, the data-set is rather ‘dirty’. The images are not properly cropped, with some side notes with different calligraphy style and some images only have one or two characters. But it’s &lt;span class="caps"&gt;OK&lt;/span&gt;. Let’s quickly train the model and see how it performs so we can gain some insights into our data.&lt;/p&gt;
&lt;h3 id="quick-and-dirty-training"&gt;Quick and Dirty Training&lt;a class="headerlink" href="#quick-and-dirty-training" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Let’s first create a model. We’ll be using transfer learning and use ResNet50 as our model. Pre-trained weights will be downloaded.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/2134/1*Zzy9-o-Q3K3BID_vZyRCoA.png"/&gt;&lt;/p&gt;
&lt;p&gt;With 3 epoches of fit_one_cycle, we managed to reach a 90% accuracy rate on our validation set. Not bad!&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/2000/1*44IMsadGzm0-mF2SdrvUKA.png"/&gt;&lt;/p&gt;
&lt;h3 id="unfreeze-and-fine-tune-our-training"&gt;Unfreeze and Fine-Tune Our Training&lt;a class="headerlink" href="#unfreeze-and-fine-tune-our-training" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Since the fit_one_cycle function will freeze the initial layers and only training the last couple of layers to speed up the training speed(this approach works because usually for transfer learning, initial layers will capture basic features of the images that are not likely to change a lot), we can hopefully further improve our accuracy by unfreezing all the layers and train again.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/2000/1*IxzL6yxuHV2nqmoXure_fg.png"/&gt;&lt;/p&gt;
&lt;p&gt;We used the above lr_find function to find a good learning rate range. The key is to find the steepest slope (as indicated in the orange circle above) in the learning curve and slice it for further training. For example, in the above figure, the bottom of the curve is at 1e-03, then we can pick one point at 1/10 of that, which is 1e-04, and the other one at 1e-06 or 1e-05 (This is inspired from an experimental concept of ‘Super-convergence’, described in details in &lt;a href="https://course.fast.ai"&gt;fast.ai course&lt;/a&gt;. Sometime you need to do a bit of experiment to find the best learning rate combination but then again, fast.ai is always preaching iterative and interactive approach.) The idea is still to train the first couple of layers slower and last couple layers faster.&lt;/p&gt;
&lt;p&gt;Let’s train another two epoch:&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/2000/1*noUlINi_AE8NZyZfJ7koBQ.png"/&gt;&lt;/p&gt;
&lt;p&gt;Slightly better and the validation_loss starts to surpass train_loss, a sign of overfitting. Let’s stop here and wrap things up.&lt;/p&gt;
&lt;h3 id="results-interpretation"&gt;Results Interpretation&lt;a class="headerlink" href="#results-interpretation" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;We reached 90% accuracy. Not state-of-the-art but already pretty impressive considering we only have a roughly 700 images per class data-set. More data will definitely lead to better results. Let’s look at our results and see if we can find some insights.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/2000/1*tyskbmlwBIE0roKxHHbseA.png"/&gt;&lt;/p&gt;
&lt;p&gt;Using the ClassificationIntepretation object from fast.ai, we can easily calculate the top_losses and see what they are:&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/2000/1*3bL7M8zSjT-PLGVTqp3hEg.png"/&gt;&lt;/p&gt;
&lt;p&gt;Look at the confusion matrix, the model does really well in recognize ‘xiaozhuan’, probably due to its unique stroke arrangements.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/2000/1*qtb-Te_AElPaO3mym-9RVw.png"/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;A couple of insights:&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;We still have totally wrong images like the grid one (2nd one on 1st row)
If there are too few (1st row, 1st column) or too many (2nd row, 2nd column) characters, the model will struggle.
Some image shows ‘in-between’ kind of styles which the model also had a hard time classify. Which is totally normal, since even human will have a hard time telling which style it belongs to.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id="final-thoughts"&gt;Final Thoughts&lt;a class="headerlink" href="#final-thoughts" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;This experimental project actually works exceedingly well with fast.ai library. &lt;a href="undefined"&gt;Jeremy Howard&lt;/a&gt; said on the course and I quote here (not exactly word by word, but I hope I captured the gist of it. 🙏):&lt;/p&gt;
&lt;blockquote&gt;
&lt;h1 id="fastai-is-a-very-opinionated-library-wherever-we-know-a-best-default-well-choose-it-for-you-whatever-best-practice-we-know-works-well-well-do-it-for-you"&gt;fast.ai is a very opinionated library. Wherever we know a best default, we’ll choose it for you. Whatever best practice we know works well, we’ll do it for you.&lt;a class="headerlink" href="#fastai-is-a-very-opinionated-library-wherever-we-know-a-best-default-well-choose-it-for-you-whatever-best-practice-we-know-works-well-well-do-it-for-you" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h1&gt;
&lt;/blockquote&gt;
&lt;p&gt;This is at least proven in this project. With only very few lines of code and very minimum efforts for data collection, we managed a 90% accurate model. I believe with more and better quality data. The state-of-the-art results could be achieved and our calligrapher classifier vision is not beyond reach.&lt;/p&gt;
&lt;p&gt;&lt;img alt="fast.ai’s tagline: Making neural nets uncool again." src="https://cdn-images-1.medium.com/max/2400/0*Yo5w5gd2_CRC1MFl.jpg"/&gt;&lt;em&gt;fast.ai’s tagline: Making neural nets uncool again.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Finally, allow me to paraphrase above tagline with a Chinese poet:&lt;/p&gt;
&lt;p&gt;&lt;img alt="“Where once the swallows knew the mansions of the great, They now to humbler homes would fly to nest and mate.“" src="https://cdn-images-1.medium.com/max/2000/1*g6k1Z7hyyeW_Y8Ge3GWmWQ.png"/&gt;&lt;em&gt;“Where once the swallows knew the mansions of the great, They now to humbler homes would fly to nest and mate.“&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;You could find out how I fine-tuned the model and achieved better accuracy at the link below:
&lt;a href="https://medium.com/datadriveninvestor/chinese-calligraphy-classifier-fine-tuning-cbfbf0e304d8"&gt;&lt;strong&gt;How I Trained Computer to Learn Calligraphy Styles: Part 2&lt;/strong&gt;
&lt;em&gt;Build a Deep Learning Models with fast.ai Library&lt;/em&gt;medium.com&lt;/a&gt;&lt;/p&gt;</content><category term="Machine Learning"></category><category term="AI"></category><category term="Deep Learning"></category><category term="fast.ai"></category><category term="calligraphy"></category></entry><entry><title>I finished Andrew Ng’s Machine Learning Course and I Felt Great!</title><link href="https://wayofnumbers.github.io/andrewng-ml-course-review.html" rel="alternate"></link><published>2019-09-06T20:00:00-05:00</published><updated>2019-09-06T20:00:00-05:00</updated><author><name>Michael Li</name></author><id>tag:wayofnumbers.github.io,2019-09-06:/andrewng-ml-course-review.html</id><summary type="html">&lt;p&gt;Thoughts and feelings for completing Andrew Ng&amp;#8217;s Machine Learning&amp;nbsp;course&lt;/p&gt;</summary><content type="html">
&lt;p&gt;The good, the bad, and the beautiful&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/2400/1*-7P4wesf7eSx46XxHV9_wg.png"/&gt;&lt;/p&gt;
&lt;p&gt;Just finished &lt;a href="undefined"&gt;Andrew Ng&lt;/a&gt;’s Machine Learning course on &lt;a href="https://www.coursera.org"&gt;Coursera&lt;/a&gt;, and it’s &lt;span class="caps"&gt;GREAT&lt;/span&gt;! Here some thoughts and observations:&lt;/p&gt;
&lt;h2 id="whats-great-about-it"&gt;What’s great about it&lt;a class="headerlink" href="#whats-great-about-it" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;em&gt;Well designed learning curve&lt;/em&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img alt="EVE Online game’s (in)famous crazy learning curve" src="https://cdn-images-1.medium.com/max/2000/1*LAjfLlfo98ej8mUIWYhnzA.jpeg"/&gt;&lt;em&gt;&lt;span class="caps"&gt;EVE&lt;/span&gt; Online game’s (in)famous crazy learning curve&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;This is especially important for people that never heard of Machine Learning. Not assuming the student have any prior knowledge and gradually guide them through complex concept makes the learning experience challenging yet still fun.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;em&gt;Avoid complex math, yet find a way to still enable student to do &lt;span class="caps"&gt;ML&lt;/span&gt; (meme)&lt;/em&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img alt="The Andrew Ng ‘Silent Protector’ Meme" src="https://cdn-images-1.medium.com/max/2000/1*YylTTtIEkCcx-EFh5g2Vsw.jpeg"/&gt;&lt;em&gt;The Andrew Ng ‘Silent Protector’ Meme&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Maybe the biggest fear for people want to get into Machine Learning and &lt;span class="caps"&gt;AI&lt;/span&gt; is ‘I’m not a math person’. Being able to not getting into too much math yet still explain clearly the concept is invaluable, especially for totally green guys.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://www.gnu.org/software/octave/"&gt;*Octave&lt;/a&gt;/&lt;a href="https://www.mathworks.com/products/matlab.html"&gt;Matlab&lt;/a&gt; is more ‘math’ like, less digression on programming language itself*&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Some people might not agree with me on this. Yes Octave/Matlab doesn’t have all the fancy libraries like &lt;a href="https://scikit-learn.org/"&gt;scikit-learn&lt;/a&gt; and Pandas, yet it’s very expressive when it comes to represent math equations. Transfer equations from class to Matlab code is easier than Python &lt;span class="caps"&gt;IMHO&lt;/span&gt;.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Cover most popular models, good foundation&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Linear/Logistic Regression&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;span class="caps"&gt;SVM&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Neural Network&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Collaborative Filtering&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Anomaly Detection&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;K-Means&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;span class="caps"&gt;PCA&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;With all these algorithms/models under your belt, you are ready to solve a lot of problems with Machine Learning.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Provide practical &lt;span class="caps"&gt;ML&lt;/span&gt; projects knowledge, not only algorithm and programming&lt;/em&gt;
Besides theory, the course also offers very practical Machine Learning project knowledge, hot to build a pipeline, how to structure the problem solving, etc.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Well designed quizzes and assignments, as part of learning too&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I was always amazed by how well the quizzes and assignments are designed. They are challenging, yet with some efforts achievable, and at the same time offer some new perspective on the lesson. I always learned a few new things doing those and totally enjoyed them.&lt;/p&gt;
&lt;h2 id="whats-lacking"&gt;What’s lacking?&lt;a class="headerlink" href="#whats-lacking" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;em&gt;A bit dated on libraries and architectures&lt;/em&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;em&gt;You won’t find the high-level &lt;a href="http://keras.io/"&gt;Keras&lt;/a&gt;, &lt;a href="https://www.tensorflow.org/"&gt;TensorFlow&lt;/a&gt; or &lt;a href="https://pytorch.org/"&gt;PyTorth&lt;/a&gt; here, but this course is about foundation of Machine Learning and it delivered on its promise.
&lt;/em&gt;2. Could use more examples/applications of &lt;span class="caps"&gt;ML&lt;/span&gt; for motivation&lt;/p&gt;
&lt;p&gt;There are a lot of exciting development and applications on &lt;span class="caps"&gt;ML&lt;/span&gt;/&lt;span class="caps"&gt;AI&lt;/span&gt; field. If students could be exposed to more of those, it will give them more reasons to keep learning.&lt;/p&gt;
&lt;h2 id="final-thoughts"&gt;Final Thoughts&lt;a class="headerlink" href="#final-thoughts" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img alt="‘Don’t worry about it if you don’t understand’ ™️" src="https://cdn-images-1.medium.com/max/2560/1*yIPIuNIn6ar7MvQnNqlWlQ.jpeg"/&gt;&lt;em&gt;‘Don’t worry about it if you don’t understand’ ™️&lt;/em&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Overall great course if you are totally new to Machine Learning. All of the well thought out contents coupled with &lt;a href="undefined"&gt;Andrew Ng&lt;/a&gt;’s gentle and calm explanation makes the learning experience a breeze and a pleasant journey. The road ahead for Machine Learning might not be so smooth after all but having a ‘soothing’ start could carry you a long way. 👍&lt;/p&gt;
&lt;/blockquote&gt;</content><category term="Machine Learning"></category><category term="AI"></category><category term="Deep Learning"></category><category term="AndrewNg"></category><category term="Coursera"></category></entry><entry><title>Types of Optimization Algorithms used in Neural Networks and Ways to Optimize Gradient Descent</title><link href="https://wayofnumbers.github.io/Typtes-of-optimization-algorithms.html" rel="alternate"></link><published>2018-02-26T17:00:00-06:00</published><updated>2018-02-26T17:00:00-06:00</updated><author><name>Internet</name></author><id>tag:wayofnumbers.github.io,2018-02-26:/Typtes-of-optimization-algorithms.html</id><summary type="html">&lt;p&gt;Types of Optimization Algorithms used in Neural Networks and Ways to Optimize Gradient&amp;nbsp;Descent&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="https://towardsdatascience.com/types-of-optimization-algorithms-used-in-neural-networks-and-ways-to-optimize-gradient-95ae5d39529f"&gt;&lt;strong&gt;Original&amp;nbsp;Story&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://towardsdatascience.com/@anishsingh20"&gt;Anish Singh Walia&lt;/a&gt;:        &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;If your input data is sparse then methods such as &lt;strong&gt;&lt;span class="caps"&gt;SGD&lt;/span&gt;,&lt;span class="caps"&gt;NAG&lt;/span&gt; and momentum&lt;/strong&gt; are inferior and perform poorly. &lt;strong&gt;For sparse data sets one should use one of the adaptive learning-rate methods.&lt;/strong&gt; An additional benefit is that we won’t need to adjust the learning rate but likely achieve the best results with the default value.
If one wants fast convergence and train a deep Neural Network Model or a highly complex Neural Network then &lt;strong&gt;Adam or any other Adaptive learning rate techniques&lt;/strong&gt; should be used because they outperforms every other optimization&amp;nbsp;algorithms.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;One thing about Machine Learning the overal depth of the topics and algorithms makes it so easy to totally &lt;em&gt;&amp;#8216;sink&amp;#8217;&lt;/em&gt; yourself into it. And there is always something to dig. This article provides a view from a higher ground and compare different optimization algorithms and their application areas, thus pulling you out of the deep hole of deep&amp;nbsp;learning. &lt;/p&gt;
&lt;p&gt;A more visual example of these algorithms, see these two beautifully crafted&amp;nbsp;animations:&lt;/p&gt;
&lt;p&gt;&lt;img alt="SGD optimization on loss surface contours" src="https://wayofnumbers.github.io/images/optimization-algorithem-1.gif" title="SGD optimization on loss surface contours"&gt;
&lt;div style="text-align: center;"&gt;&lt;span class="caps"&gt;SGD&lt;/span&gt; optimization on loss surface contours&lt;/div&gt;
&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="SGD optimization on saddle point" src="https://wayofnumbers.github.io/images/optimization-algorithem-2.gif" title="SGD optimization on saddle point"&gt;
&lt;div style="text-align: center;"&gt;&lt;span class="caps"&gt;SGD&lt;/span&gt; optimization on saddle point&lt;/div&gt;&lt;/p&gt;</content><category term="machinelearning"></category><category term="AI"></category><category term="Optimization Algorithm"></category><category term="Gradient Descent"></category><category term="Neural Networks"></category></entry><entry><title>Tweaking Pelican Elegant Theme</title><link href="https://wayofnumbers.github.io/Tweak-Pelican-Elegant-Theme.html" rel="alternate"></link><published>2018-02-24T20:00:00-06:00</published><updated>2018-02-24T20:00:00-06:00</updated><author><name>Michael Li</name></author><id>tag:wayofnumbers.github.io,2018-02-24:/Tweak-Pelican-Elegant-Theme.html</id><summary type="html">&lt;p&gt;My notes on how to tweak a Pelican theme, in this article,&amp;nbsp;Elegant.&lt;/p&gt;</summary><content type="html">
&lt;p&gt;&lt;img alt="Elegant" src="https://wayofnumbers.github.io/images/Elegant.png"/&gt;&lt;/p&gt;
&lt;p&gt;Pelican has a lot of themes, developed by the community and shared on its official GitHub repo &lt;a href="https://github.com/getpelican/pelican-themes"&gt;here&lt;/a&gt;. &lt;a href="http://www.pelicanthemes.com/"&gt;Pelican Themes&lt;/a&gt; also offer some previews of them so you can have a good idea of what to expect. 
Some themes are really easy to setup and configure, others need some efforts. The &lt;a href="http://oncrashreboot.com/elegant-best-pelican-theme-features"&gt;Elegant&lt;/a&gt; them is the latter. For most of the themes, to make it work, you just need to add define the ‘&lt;span class="caps"&gt;THEME&lt;/span&gt;’ variable, like so:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="s1"&gt;'THEME'&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;'theme/themename'&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;For Elegant, it’s way more than that, and it’s a good thing. Elegant packed a lot of great features and thorough considerations to the reader. And that’s why I choose it as the theme for my site. Good things come with a price they say. So let’s find out. &lt;/p&gt;
&lt;h2 id="search"&gt;Search&lt;a class="headerlink" href="#search" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Search is useful when you have a lot of articles. All serious blog need to have it. To use it, add ‘tipue_search’ and ‘sitemap’ to your plugins and it will automatically be enabled. &lt;/p&gt;
&lt;h2 id="about-me-and-my-project"&gt;About Me and My Project&lt;a class="headerlink" href="#about-me-and-my-project" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Elegant’s home page layout put the blogger himself front and center with ‘About Me’ and the ‘My Project’ at the top, followed with ‘Recent Posts’. To use them, you need to set the ‘LANDING_PAGE_ABOUT’ and ‘&lt;span class="caps"&gt;PROJECTS&lt;/span&gt;’ variables in the &lt;code&gt;pelicanconf.py&lt;/code&gt;. &lt;/p&gt;
&lt;h2 id="jquery-issue"&gt;jQuery Issue&lt;a class="headerlink" href="#jquery-issue" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;I’ve enabled all the nice features, like search, collasible comments, collasible comments. But they all won’t work on Chrome because it’s considered ‘unsafe scripts’. After some digging, it turns out the site is using &lt;span class="caps"&gt;HTTPS&lt;/span&gt;, while the original theme’s template uses &lt;span class="caps"&gt;HTTP&lt;/span&gt; to load the jQuery that did all these nice features. Once I replaced the &lt;span class="caps"&gt;HTTP&lt;/span&gt; with its &lt;span class="caps"&gt;HTTPS&lt;/span&gt; counterpart, everything works like a charm. &lt;/p&gt;
&lt;h2 id="table-of-contents"&gt;Table of Contents&lt;a class="headerlink" href="#table-of-contents" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Took me some time to get table of contents to work. Firstly ‘extract_toc’ plugin needs to be added into the ‘&lt;span class="caps"&gt;PLUGINS&lt;/span&gt;’ variable. Then ‘markdown’ Python module needs to be installed and configured for it to work as the Elegant website instructions. But after all this, it still didn’t work. Turns out, you need to add &lt;code&gt;[TOC]&lt;/code&gt; in the Markdown file, after all the meta data, to actually add the table of contents into your post. After I did that, everything works. &lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;a class="headerlink" href="#conclusion" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Install and tweaking a Pelican theme isn’t that hard. Look into the static folder for &lt;span class="caps"&gt;CSS&lt;/span&gt;, tweak them if you want, or add custom &lt;span class="caps"&gt;CSS&lt;/span&gt; of your own and load them in the template. Then go into the template folder to check the html files. With basic &lt;span class="caps"&gt;HTML&lt;/span&gt;/&lt;span class="caps"&gt;CSS&lt;/span&gt;/Javascripts knowledge, you already can achieve a lot on tweaking any theme of your liking. &lt;/p&gt;</content><category term="Pelican"></category><category term="Blog"></category><category term="Github"></category><category term="Theme"></category></entry><entry><title>The AI Shortage</title><link href="https://wayofnumbers.github.io/The-AI-Shortage.html" rel="alternate"></link><published>2018-02-23T17:00:00-06:00</published><updated>2018-02-23T17:00:00-06:00</updated><author><name>Internet</name></author><id>tag:wayofnumbers.github.io,2018-02-23:/The-AI-Shortage.html</id><summary type="html">&lt;p&gt;&lt;span class="caps"&gt;AI&lt;/span&gt; talent shortage is not getting better any time&amp;nbsp;soon&amp;#8230;&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="https://medium.com/@Moscow25/the-ai-talent-shortage-704d8cf0c4cc"&gt;&lt;strong&gt;Original&amp;nbsp;Story&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://medium.com/@Moscow25"&gt;Nikolai Yakovenko&lt;/a&gt; from &lt;span class="caps"&gt;NVIDIA&lt;/span&gt;:        &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;But when I look for a designer, a Java developer, a real estate agent, etc — some are way better than others and deserve to get paid more than an &lt;span class="caps"&gt;AI&lt;/span&gt; researcher — but you’re fundamentally talking about pulling from a large well-balanced pool. It’s mostly an information game, and a matter of getting a little better than you need, but not much more than you can afford or should be paying.
In &lt;span class="caps"&gt;AI&lt;/span&gt;, it’s different. There just aren’t enough people to go around. And there aren’t enough people for every good project that can be attempted. Either academic, or something that if it works, can save the company&amp;nbsp;$1M.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The booming of a new disruptive technology always did this to the industry as well as the talent pool. It drives money into investing on the next big thing, and the money lures more talents into the field. There will be a shortage in the very beginning, and there will always be a surplus at the end of the curve. I&amp;#8217;m afraid &lt;span class="caps"&gt;AI&lt;/span&gt; won&amp;#8217;t be any different. It&amp;#8217;s just that the curve will take 10 maybe more years to unfold so it&amp;#8217;s not too late to get in the game if you think you have the stuff, since at the end of day, people with talent and grit will win, in every new technology &amp;#8216;gold&amp;nbsp;rush&amp;#8217;. &lt;/p&gt;</content><category term="machinelearning"></category><category term="AI"></category></entry><entry><title>Setup Data Science Blog with Pelican + GitHub Pages</title><link href="https://wayofnumbers.github.io/Setup-Pelican-1.html" rel="alternate"></link><published>2018-02-14T20:00:00-06:00</published><updated>2018-02-14T20:00:00-06:00</updated><author><name>Michael Li</name></author><id>tag:wayofnumbers.github.io,2018-02-14:/Setup-Pelican-1.html</id><summary type="html">&lt;p&gt;My notes on how to setup Data Science blog using Pelican static site generater and GitHub&amp;nbsp;Pages.&lt;/p&gt;</summary><content type="html">
&lt;p&gt;&lt;img alt="Coding Background" src="https://wayofnumbers.github.io/images/coding.png"/&gt;&lt;/p&gt;
&lt;p&gt;First of all, this is by no means a thorough tutorial. I’ve followed Dataquest’s blog post: &lt;a href="https://www.dataquest.io/blog/how-to-setup-a-data-science-blog/"&gt;Building a data science portfolio: Making a data science blog&lt;/a&gt; to get this one setup. Here are some insights and hiccups that may be helpful to others who want to do the same thing.&lt;/p&gt;
&lt;h2 id="static-sites-and-static-sites-generator"&gt;Static sites and static sites generator&lt;a class="headerlink" href="#static-sites-and-static-sites-generator" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;If you have never experienced the web development world, static site might be a new word to you. Actually it’s quite simple, it’s just plan web-site with &lt;span class="caps"&gt;HTML&lt;/span&gt; files, &lt;span class="caps"&gt;CSS&lt;/span&gt; sheets and Javascript files. These file never changes unless you make them, thus the word ‘static’. The ‘dynamic’ site, on the other hand, use database and complex post-end technology to ‘dynamically’ generate these &lt;span class="caps"&gt;HTML&lt;/span&gt;/&lt;span class="caps"&gt;CSS&lt;/span&gt;/Javascripts files. It’s much harder to develop and maintain. 
But I don’t want that complexity you say. I just want to write something and post them and make them look neat. Then, my friend, look no further than a static site. Good news to us, there are a lot of static sites generators out there that can help us do the heavy-lifting of developing a website. 
The static sites generators come with many flavors, &lt;a href="https://jekyllrb.com/"&gt;Jekyell(based on Ruby)&lt;/a&gt;, &lt;a href="https://blog.getpelican.com/"&gt;Pelican(based on Python&lt;/a&gt; are too popular one. Since I’m more familiar with Python. I decided to use Pelican to build my data science blog.&lt;/p&gt;
&lt;p&gt;The beautiful thing here is, since Pelican is written in Python, it’s quite easy to make it work with Jupyter Notebook, which is a huge bonus for data science. This means you can write your blog posts using Jupyter Notebook, leverage all the powerful snippets, data visualization and code executing it has and roll all those into your post, with ease.&lt;/p&gt;
&lt;h2 id="install-pelican"&gt;Install Pelican&lt;a class="headerlink" href="#install-pelican" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Usually install Pelican will be easy, but if we also want to support Jupyter Notebook it will be harder. Many python modules will need to be installed using &lt;strong&gt;pip&lt;/strong&gt;. &lt;/p&gt;
&lt;p&gt;Here is a list I used:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nv"&gt;Markdown&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="m"&gt;2&lt;/span&gt;.6.6    &lt;span class="c1"&gt;# Markdown support&lt;/span&gt;
&lt;span class="nv"&gt;pelican&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="m"&gt;3&lt;/span&gt;.6.3     &lt;span class="c1"&gt;# Pelican itself&lt;/span&gt;
jupyter&amp;gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;.0       &lt;span class="c1"&gt;# Jupyter Notebook&lt;/span&gt;
ipython&amp;gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;4&lt;/span&gt;.0       &lt;span class="c1"&gt;# iPython&lt;/span&gt;
nbconvert&amp;gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;4&lt;/span&gt;.0     &lt;span class="c1"&gt;#&lt;/span&gt;
beautifulsoup4     &lt;span class="c1"&gt;# not sure why we need pharsing here, maybe manipulating codes&lt;/span&gt;
ghp-import&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;.4.1  &lt;span class="c1"&gt;#handle git branches&lt;/span&gt;
&lt;span class="nv"&gt;matplotlib&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;.5.1  &lt;span class="c1"&gt;#data visualization&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Once all are installed, run:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;pelican-quickstart
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Answer couple of questions and the backbone of your site is up. To make the Jupyter Notebook part work, we will need this Pelican plugin (yes, Pelican support plugins!): &lt;a href="https://github.com/danielfrg/pelican-ipynb"&gt;Pelican-ipynb&lt;/a&gt;. 
Once installed, activate the plugin in your &lt;code&gt;pelicanconf.py&lt;/code&gt;. This is your dot file, and you’ll be dealig with it a lot later on. 
Add these into the bottom:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;MARKUP&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'md'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'ipynb'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;PLUGIN_PATH&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;'./plugins'&lt;/span&gt;
&lt;span class="n"&gt;PLUGINS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'ipynb.markup'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="write-post"&gt;Write Post&lt;a class="headerlink" href="#write-post" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Well this is the easier part. Just put your Jupyter Notebook file into the &lt;code&gt;'content'&lt;/code&gt; folder. Also, for each post, we’ll need a meta file to include some meta data of the post. The meta file should have the extension: &lt;code&gt;.ipynb-meta&lt;/code&gt;. Here is an example:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;Title&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;First&lt;/span&gt; &lt;span class="n"&gt;Post&lt;/span&gt;
&lt;span class="n"&gt;Slug&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;first&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;post&lt;/span&gt;
&lt;span class="n"&gt;Date&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;2016&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;06&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;08&lt;/span&gt; &lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;00&lt;/span&gt;
&lt;span class="n"&gt;Category&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;posts&lt;/span&gt;
&lt;span class="n"&gt;Tags&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;python&lt;/span&gt; &lt;span class="n"&gt;firsts&lt;/span&gt;
&lt;span class="n"&gt;author&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Vik&lt;/span&gt; &lt;span class="n"&gt;Paruchuri&lt;/span&gt;
&lt;span class="n"&gt;Summary&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;My&lt;/span&gt; &lt;span class="n"&gt;first&lt;/span&gt; &lt;span class="n"&gt;post&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="n"&gt;read&lt;/span&gt; &lt;span class="n"&gt;it&lt;/span&gt; &lt;span class="n"&gt;to&lt;/span&gt; &lt;span class="n"&gt;find&lt;/span&gt; &lt;span class="n"&gt;out&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;It’s quite easy to figure out what they are so I won’t bother explain here. When done, save. &lt;/p&gt;
&lt;h2 id="generating-html"&gt;Generating &lt;span class="caps"&gt;HTML&lt;/span&gt;&lt;a class="headerlink" href="#generating-html" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Exit out of content folder, and run &lt;code&gt;pelican content&lt;/code&gt; to generate the &lt;span class="caps"&gt;HTML&lt;/span&gt;. Enter &lt;code&gt;output&lt;/code&gt; again and run:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;python -m pelican.server
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Then visit: &lt;code&gt;localhost:8000&lt;/code&gt; to see your new site. &lt;/p&gt;
&lt;h2 id="putting-it-on-github-pages"&gt;Putting it on GitHub Pages&lt;a class="headerlink" href="#putting-it-on-github-pages" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Create a GitHub Page is simple and there are many tutorials out there. Once created, edit your &lt;code&gt;SITEURL&lt;/code&gt; in &lt;code&gt;publishconf.py&lt;/code&gt; file, make it into &lt;code&gt;https://username.github.io&lt;/code&gt;, substitute &lt;code&gt;username&lt;/code&gt; with your site name. &lt;/p&gt;
&lt;hr/&gt;
&lt;p&gt;Run &lt;code&gt;pelican content -s publishconf.py&lt;/code&gt; to generate the real stuff. &lt;/p&gt;
&lt;p&gt;Run &lt;code&gt;ghp-import output -b master&lt;/code&gt; to import everything into the &lt;code&gt;output&lt;/code&gt; folder to the &lt;code&gt;master&lt;/code&gt; branch. &lt;/p&gt;
&lt;p&gt;Run &lt;code&gt;git push origin master&lt;/code&gt; to push changes to GitHub repo. &lt;/p&gt;
&lt;hr/&gt;
&lt;h2 id="themes"&gt;Themes&lt;a class="headerlink" href="#themes" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;There are a lot of &lt;a href="https://github.com/getpelican/pelican-themes"&gt;themes&lt;/a&gt; to choose from. What you need to do is to configure your &lt;code&gt;pelicanconf.py&lt;/code&gt; file and assign the theme name. Some themes may need to install extra Python modules or have access to other services to work. But overall the process is straight forward.&lt;/p&gt;
&lt;h2 id="google-analytics"&gt;Google Analytics&lt;a class="headerlink" href="#google-analytics" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Pelican have Google Analytics support out of the box. Register the site on &lt;span class="caps"&gt;GA&lt;/span&gt;, then get the &lt;code&gt;UA-XXXXxxxxx&lt;/code&gt; id, put it into the &lt;code&gt;pelicanconf.py&lt;/code&gt; file and you’re golden. &lt;/p&gt;
&lt;h2 id="disqus"&gt;Disqus&lt;a class="headerlink" href="#disqus" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Disqus support come out of the box too. Register the site on Disqus, get your &lt;strong&gt;shortname&lt;/strong&gt; correct, and put into &lt;code&gt;pelicanconf.py&lt;/code&gt; and you should be good too. Some turorial suggest put into &lt;code&gt;publishconf.py&lt;/code&gt;, well mine only works on &lt;code&gt;pelicanconf.py&lt;/code&gt; so use your own judgement. &lt;/p&gt;
&lt;h2 id="seo"&gt;&lt;span class="caps"&gt;SEO&lt;/span&gt;&lt;a class="headerlink" href="#seo" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Basic &lt;span class="caps"&gt;SEO&lt;/span&gt; can be achieved using &lt;strong&gt;sitemap&lt;/strong&gt; plugin. Search for it and put into &lt;code&gt;pelicanconf.py&lt;/code&gt;, it will work automatically.&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;a class="headerlink" href="#conclusion" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Overall the process is not hard at all. Once everything is set. Just focus on putting in solid content using Jupyter Notebook. Enjoy coding, visualizing and writing!&lt;/p&gt;</content><category term="Pelican"></category><category term="Data Science"></category><category term="Blog"></category><category term="Github"></category><category term="Jupyter Notebook"></category><category term="Disqus"></category><category term="Google Analytics"></category></entry></feed>