{"pages":[{"title":"About","text":"Data science blog for the rest of us. Michael Li Designer, Developer. Xiao Xu Deep thinker.","tags":"about","url":"pages/about.html"},{"title":"How I Trained Computer to Learn Calligraphy Styles: Part 2","text":"Build a Deep Learning Model with fast.ai Library Photo by Kon Karampelas on Unsplash I wanted to start a series of posts for the projects I finished/polished for my Practical Deep Learning for Coders fast.ai course. Since I'm pretty green on ML / DL field, I hope the challenges I faced and overcome could be of value for other people experiencing the same journey. Model 1 ・ 1a Making It Even Better In my last post , I explained the approach I take for this image recognition problem using fast.ai library. As you can see, once we get the data down to a fast.ai ImageDataBunch, the code is rather simple and we achieve a 90% accuracy rate, which is quite impressive considering the quality of our data(randomly downloaded from Google/Baidu search without much data cleaning). Now, can we do better? DDI Editor's Pick: 5 Machine Learning Books That Turn You from Novice to Expert | Data Driven… The booming growth in the Machine Learning industry has brought renewed interest in people about Artificial… www.datadriveninvestor.com Turns out, we can! How? Well, there are two things in our prior pipeline that could improve: Image Pre-processing Tweak Model Training Fine Tune. Let's dive deeper. Image Pre-Processing Tweak Remember when we import our data into fast.ai ImageDataBunch, we used the following code: Notice that on our image pre-processing, i.e. get_transforms function, we didn't give it any parameter and just used the default. The default will try to apply a variety of image augmentation techniques to make the image data-set generalize better, like flipping, warping, rotating, cropping, etc. This is good, fast.ai library helped us do the ‘best practice' for the majority of the cases. But in our case here, some default might not work that well. The biggest one is ‘flipping'. Because we are trying to classify calligraphy artworks and in real life, it will never randomly flip left/right or up/down. So making the images flips randomly will not reflect the real-life cases and thus won't help with our training accuracy. To fix this, we tweaked our code as below: Notice we pass do_flip=False into the get_transforms function, thus telling the module to not randomly flipping our images during importing. Model Training Fine Tune Now that the image pre-processing is done. We can re-structure out model training to avoid overfitting and achieve better accuracy. This approach is introduced in the fast.ai Practical Deep Learning for Coders course lesson 3 . Instead of training the model directly from a 256x256 image size, we'll gradually scaling up the image size. More concretely, we will first train a CNN to classify the images of 128x128 size, once we achieved best accuracy, we'll then use transfer learning and keep training the model on the same data-set, except with 256x256 image size. We'll call the 128x128 image size training ‘stage 1' and 256x256 image size training ‘stage 2' After our stage 1 training(where my last post left off), we have a trained CNN model called learn , it's ‘unfreezed' and achieves an accuracy of around 85%. Accuracy 86% after training a 128x128 image size CNN . Now we need to freeze the network again, create a new ImageDataBunch with 256x256 image size and restart the same training process. After finding the best learning rate, we train the CNN with another 2 epochs, already breaking into 91% accuracy. We'll then do the same ‘unfreeze' and keep training. After unfreeze, we trained the model with another 4 epochs, the accuracy broke into 96.5% . Observed that valudation_losshas already surpassed training_loss, suggesting a sign of overfitting. We'll stop our training here. This simple technique is also called ‘ Progressive resizing ' by Jeremy Howard from fast.ai and helped his team beat Google in a competition of speed training IMAGENET in *DAWNBench by training the IMAGGNET in a whopping 18 minutes and \\$40 Amazon AWS cost.* To Wrap It Up Photo by Franki Chamaki on Unsplash With two simple tweaks, we managed to increase the accuracy around 6.5%, breaking into the state-of-the-art range of results. Major takeaways: When doing image pre-processing, make sure the processed images still properly represent what real-life data will look like. The reason gradually increase training image size works is: by giving the trained model a data-set that's 4 times bigger, actually means giving the model a brand new data to train, avoiding overfitting. Starting from smaller sized images for training will also have the benefit of faster training and quicker experimenting. This usually leads to better results. That's it for Chinese Calligraphy Classifier. I hope you learned a thing or two after reading these two articles. We're trying to get some specific calligrapher's ‘true' and ‘fake' artworks and see if we can build a ‘true or false' classifier. This will be a very interesting and much valuable next step. Will report back and write more articles if we made real progress. But until then, we'll move on to put this well-trained model into production and build a web-app around it. Stay tuned. If you haven't read my first post on this topic, here's the link: How I Trained Computer to Learn Calligraphy Styles: Part1 Build a Deep Learning Model with fast.ai Library medium.com","tags":"Machine Learning","url":"chinese-calligraphy-classifier-2.html"},{"title":"Types of Optimization Algorithms used in Neural Networks and Ways to Optimize Gradient Descent","text":"Original Story Anish Singh Walia : If your input data is sparse then methods such as SGD , NAG and momentum are inferior and perform poorly. For sparse data sets one should use one of the adaptive learning-rate methods. An additional benefit is that we won't need to adjust the learning rate but likely achieve the best results with the default value. If one wants fast convergence and train a deep Neural Network Model or a highly complex Neural Network then Adam or any other Adaptive learning rate techniques should be used because they outperforms every other optimization algorithms. One thing about Machine Learning the overal depth of the topics and algorithms makes it so easy to totally ‘sink' yourself into it. And there is always something to dig. This article provides a view from a higher ground and compare different optimization algorithms and their application areas, thus pulling you out of the deep hole of deep learning. A more visual example of these algorithms, see these two beautifully crafted animations: SGD optimization on loss surface contours SGD optimization on saddle point","tags":"Machine Learning","url":"Typtes-of-optimization-algorithms.html"},{"title":"Tweaking Pelican Elegant Theme","text":"[ TOC ] Pelican has a lot of themes, developed by the community and shared on its official GitHub repo here . Pelican Themes also offer some previews of them so you can have a good idea of what to expect. Some themes are really easy to setup and configure, others need some efforts. The Elegant them is the latter. For most of the themes, to make it work, you just need to add define the ‘ THEME ' variable, like so: 'THEME' = 'theme/themename' For Elegant, it's way more than that, and it's a good thing. Elegant packed a lot of great features and thorough considerations to the reader. And that's why I choose it as the theme for my site. Good things come with a price they say. So let's find out. Search Search is useful when you have a lot of articles. All serious blog need to have it. To use it, add ‘tipue_search' and ‘sitemap' to your plugins and it will automatically be enabled. About Me and My Project Elegant's home page layout put the blogger himself front and center with ‘About Me' and the ‘My Project' at the top, followed with ‘Recent Posts'. To use them, you need to set the ‘LANDING_PAGE_ABOUT' and ‘ PROJECTS ' variables in the pelicanconf.py . jQuery Issue I've enabled all the nice features, like search, collasible comments, collasible comments. But they all won't work on Chrome because it's considered ‘unsafe scripts'. After some digging, it turns out the site is using HTTPS , while the original theme's template uses HTTP to load the jQuery that did all these nice features. Once I replaced the HTTP with its HTTPS counterpart, everything works like a charm. Table of Contents Took me some time to get table of contents to work. Firstly ‘extract_toc' plugin needs to be added into the ‘ PLUGINS ' variable. Then ‘markdown' Python module needs to be installed and configured for it to work as the Elegant website instructions. But after all this, it still didn't work. Turns out, you need to add [TOC] in the Markdown file, after all the meta data, to actually add the table of contents into your post. After I did that, everything works. Conclusion Install and tweaking a Pelican theme isn't that hard. Look into the static folder for CSS , tweak them if you want, or add custom CSS of your own and load them in the template. Then go into the template folder to check the html files. With basic HTML / CSS /Javascripts knowledge, you already can achieve a lot on tweaking any theme of your liking.","tags":"Tools","url":"Tweak-Pelican-Elegant-Theme.html"},{"title":"The AI Shortage","text":"Original Story Nikolai Yakovenko from NVIDIA : But when I look for a designer, a Java developer, a real estate agent, etc — some are way better than others and deserve to get paid more than an AI researcher — but you're fundamentally talking about pulling from a large well-balanced pool. It's mostly an information game, and a matter of getting a little better than you need, but not much more than you can afford or should be paying. In AI , it's different. There just aren't enough people to go around. And there aren't enough people for every good project that can be attempted. Either academic, or something that if it works, can save the company $1M. The booming of a new disruptive technology always did this to the industry as well as the talent pool. It drives money into investing on the next big thing, and the money lures more talents into the field. There will be a shortage in the very beginning, and there will always be a surplus at the end of the curve. I'm afraid AI won't be any different. It's just that the curve will take 10 maybe more years to unfold so it's not too late to get in the game if you think you have the stuff, since at the end of day, people with talent and grit will win, in every new technology ‘gold rush'.","tags":"Machine Learning","url":"The-AI-Shortage.html"},{"title":"Setup Data Science Blog with Pelican + GitHub Pages","text":"[ TOC ] First of all, this is by no means a thorough tutorial. I've followed Dataquest's blog post: Building a data science portfolio: Making a data science blog to get this one setup. Here are some insights and hiccups that may be helpful to others who want to do the same thing. Static sites and static sites generator If you have never experienced the web development world, static site might be a new word to you. Actually it's quite simple, it's just plan web-site with HTML files, CSS sheets and Javascript files. These file never changes unless you make them, thus the word ‘static'. The ‘dynamic' site, on the other hand, use database and complex post-end technology to ‘dynamically' generate these HTML / CSS /Javascripts files. It's much harder to develop and maintain. But I don't want that complexity you say. I just want to write something and post them and make them look neat. Then, my friend, look no further than a static site. Good news to us, there are a lot of static sites generators out there that can help us do the heavy-lifting of developing a website. The static sites generators come with many flavors, Jekyell(based on Ruby) , Pelican(based on Python are too popular one. Since I'm more familiar with Python. I decided to use Pelican to build my data science blog. The beautiful thing here is, since Pelican is written in Python, it's quite easy to make it work with Jupyter Notebook, which is a huge bonus for data science. This means you can write your blog posts using Jupyter Notebook, leverage all the powerful snippets, data visualization and code executing it has and roll all those into your post, with ease. Install Pelican Usually install Pelican will be easy, but if we also want to support Jupyter Notebook it will be harder. Many python modules will need to be installed using pip . Here is a list I used: Markdown == 2 .6.6 # Markdown support pelican == 3 .6.3 # Pelican itself jupyter> = 1 .0 # Jupyter Notebook ipython> = 4 .0 # iPython nbconvert> = 4 .0 # beautifulsoup4 # not sure why we need pharsing here, maybe manipulating codes ghp-import == 0 .4.1 #handle git branches matplotlib == 1 .5.1 #data visualization Once all are installed, run: pelican-quickstart Answer couple of questions and the backbone of your site is up. To make the Jupyter Notebook part work, we will need this Pelican plugin (yes, Pelican support plugins!): Pelican-ipynb . Once installed, activate the plugin in your pelicanconf.py . This is your dot file, and you'll be dealig with it a lot later on. Add these into the bottom: MARKUP = ( 'md' , 'ipynb' ) PLUGIN_PATH = './plugins' PLUGINS = [ 'ipynb.markup' ] Write Post Well this is the easier part. Just put your Jupyter Notebook file into the 'content' folder. Also, for each post, we'll need a meta file to include some meta data of the post. The meta file should have the extension: .ipynb-meta . Here is an example: Title : First Post Slug : first - post Date : 2016 - 06 - 08 20 : 00 Category : posts Tags : python firsts author : Vik Paruchuri Summary : My first post , read it to find out . It's quite easy to figure out what they are so I won't bother explain here. When done, save. Generating HTML Exit out of content folder, and run pelican content to generate the HTML . Enter output again and run: python -m pelican.server Then visit: localhost:8000 to see your new site. Putting it on GitHub Pages Create a GitHub Page is simple and there are many tutorials out there. Once created, edit your SITEURL in publishconf.py file, make it into https://username.github.io , substitute username with your site name. Run pelican content -s publishconf.py to generate the real stuff. Run ghp-import output -b master to import everything into the output folder to the master branch. Run git push origin master to push changes to GitHub repo. Themes There are a lot of themes to choose from. What you need to do is to configure your pelicanconf.py file and assign the theme name. Some themes may need to install extra Python modules or have access to other services to work. But overall the process is straight forward. Google Analytics Pelican have Google Analytics support out of the box. Register the site on GA , then get the UA-XXXXxxxxx id, put it into the pelicanconf.py file and you're golden. Disqus Disqus support come out of the box too. Register the site on Disqus, get your shortname correct, and put into pelicanconf.py and you should be good too. Some turorial suggest put into publishconf.py , well mine only works on pelicanconf.py so use your own judgement. SEO Basic SEO can be achieved using sitemap plugin. Search for it and put into pelicanconf.py , it will work automatically. Conclusion Overall the process is not hard at all. Once everything is set. Just focus on putting in solid content using Jupyter Notebook. Enjoy coding, visualizing and writing!","tags":"Tools","url":"Setup-Pelican-1.html"}]}