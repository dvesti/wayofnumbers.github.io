<!DOCTYPE html>
<html lang="en">
<head>
          <title>Way of Numbers - OpenAI: Catch Me If You&nbsp;Can</title>
        <meta charset="utf-8" />
        <link href="https://wayofnumbers.github.io/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Way of Numbers Full Atom Feed" />
        <link href="https://wayofnumbers.github.io/feeds/machine-learning.atom.xml" type="application/atom+xml" rel="alternate" title="Way of Numbers Categories Atom Feed" />




    <meta name="tags" content="Machine Learning" />
    <meta name="tags" content="AI" />
    <meta name="tags" content="OpenAI" />
    <meta name="tags" content="Gaming" />
    <meta name="tags" content="Reinforcement Learning" />

</head>

<body id="index" class="home">
        <header id="banner" class="body">
                <h1><a href="https://wayofnumbers.github.io/">Way of Numbers <strong>Data science for the rest of us.</strong></a></h1>
        </header><!-- /#banner -->
        <nav id="menu"><ul>
            <li><a href="/">Homepage</a></li>
            <li><a href="/categories.html">Categories</a></li>
            <li><a href="./pages/about.html">About</a></li>
            <li class="active"><a href="https://wayofnumbers.github.io/category/machine-learning.html">Machine Learning</a></li>
            <li><a href="https://wayofnumbers.github.io/category/tools.html">Tools</a></li>
        </ul></nav><!-- /#menu -->
<section id="content" class="body">
  <header>
    <h2 class="entry-title">
      <a href="https://wayofnumbers.github.io/openai-catch-me-if-you-can.html" rel="bookmark"
         title="Permalink to OpenAI: Catch Me If You Can">OpenAI: Catch Me If You&nbsp;Can</a></h2>
 
  </header>
  <footer class="post-info">
    <time class="published" datetime="2019-09-19T20:00:00-05:00">
      Thu 19 September 2019
    </time>
    <address class="vcard author">
      By           <a class="url fn" href="https://wayofnumbers.github.io/author/michael-li.html">Michael Li</a>
    </address>
    <div class="category">
        Category: <a href="https://wayofnumbers.github.io/category/machine-learning.html">Machine Learning</a>
    </div>
    <div class="tags">
        Tags:
            <a href="https://wayofnumbers.github.io/tag/machine-learning.html">Machine Learning</a>
            <a href="https://wayofnumbers.github.io/tag/ai.html">AI</a>
            <a href="https://wayofnumbers.github.io/tag/openai.html">OpenAI</a>
            <a href="https://wayofnumbers.github.io/tag/gaming.html">Gaming</a>
            <a href="https://wayofnumbers.github.io/tag/reinforcement-learning.html">Reinforcement Learning</a>
    </div>
  </footer><!-- /.post-info -->
  <div class="entry-content">
    
<p>What OpenAI’s Multi-Agent Hide and Seek Break Through Means</p>
<p><img alt="" src="https://cdn-images-1.medium.com/max/5074/1*zx1DVdwYOVJWdHURXr5qjw.png"/></p>
<h3 id="who-is-openai">Who is OpenAI?<a class="headerlink" href="#who-is-openai" title="Permanent link">¶</a></h3>
<p>When it comes to reinforcement learning, OpenAI is a big name. The <a href="https://gym.openai.com/">OpenAI Gym toolkit</a> provides a solid foundation for a lot of <span class="caps">ML</span> researchers to explore and study reinforcement learning techniques. They also are known to have developed ‘<a href="https://openai.com/blog/better-language-models/"><span class="caps">GPT</span>-2</a>’ language model. The ‘deep fake’ news the model generated is so scarily good that OpenAI refused to release the trained model, just the <a href="https://github.com/openai/gpt-2">code </a>and <a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">paper</a>.</p>
<p><img alt="From OpenAI.com" src="https://cdn-images-1.medium.com/max/2406/1*p3ZS8EKITYRCTu1_JqtwiA.png"/><em>From OpenAI.com</em></p>
<h3 id="hide-and-seek-only-this-time-the-computer-is-playing-it">Hide and Seek, Only This Time the Computer is Playing It<a class="headerlink" href="#hide-and-seek-only-this-time-the-computer-is-playing-it" title="Permanent link">¶</a></h3>
<p>Today they release something new and equally groundbreaking: <a href="https://openai.com/blog/emergent-tool-use/">The Multi-agent Hide and Seek</a>. You can have a look at the short video below from their website:</p>
<p><center><iframe allowfullscreen="" frameborder="0" height="315" src="https://www.youtube.com/embed/kopoLzvh5jY" width="560"></iframe></center></p>
<p>Simply put, they designed a reinforcement agent and let it play a simple ‘Hide and Seek’ game that we all played when we were still a kid. After millions of games played with itself and the prior version of itself. Both the seeking and hiding agent developed several very effective strategies to counter each other and win.</p>
<p><img alt="Basic abilities agent has to play and win the game" src="https://cdn-images-1.medium.com/max/2000/1*ldh0SVwH9-Z8iZWY_MGhTw.gif"/><em>Basic abilities agent has to play and win the game</em></p>
<h3 id="how-to-train-your-dragon-urr-agent">How to Train Your Dragon, urr… Agent<a class="headerlink" href="#how-to-train-your-dragon-urr-agent" title="Permanent link">¶</a></h3>
<p>The agents develop their tactics in an adversarial way.</p>
<ol>
<li>
<p>Both agents move randomly</p>
</li>
<li>
<p>Seeking agent learns to chase hiding agent.</p>
</li>
<li>
<p>Hiding agent learns to run away from seeking agent (not working)</p>
</li>
<li>
<p>Hiding agent learns to use blocks in the environment to block entrance to a small room and hide inside (It works!)</p>
</li>
<li>
<p>Seeking agent learns to use ramp blocks in the environment to jump over the wall to get inside the blocked room</p>
</li>
<li>
<p>Hiding agent learns to move ramp blocks inside the room, then block the room</p>
</li>
<li>
<p>…… the game keeps playing and more gaming mechanism is discovered and exploited, both agents getting smarter and smarter.</p>
</li>
</ol>
<p>The seeking agent and hiding agent counter each other and at the same time grow with each other. This is a concept (相生相克 mutually reinforce and neutralize each other) very familiar to Chinese people, and can be illustrated in a simple way:</p>
<p><img alt="Taichi 太极" src="https://cdn-images-1.medium.com/max/2000/1*w3rxOO7r5d96JdDDAYskpQ.gif"/><em>Taichi 太极</em></p>
<p>Philosophy aside, we did see this kind of concept appear on recent <span class="caps">AI</span> field quite often. Another example is the rise of <a href="https://towardsdatascience.com/must-read-papers-on-gans-b665bbae3317"><span class="caps">GAN</span> (General Adversarial Network)</a> where a ‘generator’ and ‘discriminator’ are trained at the same time to achieve state-of-the-art results.</p>
<h3 id="the-agents-looks-cute-but-why-i-still-feels-a-bit-chilly-on-my-spine">The Agents Looks Cute! But Why I Still Feels a Bit Chilly on My Spine?<a class="headerlink" href="#the-agents-looks-cute-but-why-i-still-feels-a-bit-chilly-on-my-spine" title="Permanent link">¶</a></h3>
<p>Agents in the game are quite cute with cartoony big heads and smiley eyes. But underneath the cuteness, what does the great results suggest? Well, just imagine, if they are not playing this cute little hide and seek game where agents giggles when get caught, rather, they are playing Doom or Quake, where blood and gores fly around when the agent gets caught. Will the bloody scene lead you to start worrying about the possible application of this model and the potential it has if weaponized? If this still seems too far away from reality, let me bring this uncomfortable imagination one step further, allow me to use three words:</p>
<blockquote>
<h1 id="boston-dynamic-drones-skynet">Boston Dynamic, Drones, Skynet.<a class="headerlink" href="#boston-dynamic-drones-skynet" title="Permanent link">¶</a></h1>
</blockquote>
<p><center><iframe allowfullscreen="" frameborder="0" height="315" src="https://www.youtube.com/embed/dKjCWfuvYxQ" width="560"></iframe></center></p>
<p>The tasks and tactics agent learned from millions of games might still seem easy. Hide, use blocks, use ramps, etc. But don’t forget that complicated and sophisticated strategy is formed with all these small pieces. One big advancement of <span class="caps">AI</span> recently is <a href="https://en.wikipedia.org/wiki/Transfer_learning">transfer learning</a>, build new <span class="caps">AI</span> models on top of already trained/learned models. (Using transfer learning based on already trained <a href="http://www.image-net.org/"><span class="caps">IMAGENET</span></a> model, people can quickly train a fine-grained cat/dog classifier with only 100 images and 1 <span class="caps">GPU</span> in minutes. I explained the approach of <a href="https://course.fast.ai/videos/?lesson=1">fast.ai</a> at <a href="https://medium.com/datadriveninvestor/deep-learning-models-by-fast-ai-library-c1cccc13e2b3">here</a>). These basic game tactic model can be utilized in the future to build more realistic and dangerous military strategy models that can totally be applied in war.<strong> This is not beyond our reach now. </strong>If we put all our current <span class="caps">AI</span> and robotic achievements together, great/scary things can be achieved.</p>
<blockquote>
<p>When an OpenAI model agent running within a Boston Dynamics robot or killer drones, and video surveillance networks everywhere to watch your every step, if you are the hider playing this game, what is the chance of you winning?</p>
</blockquote>
  </div><!-- /.entry-content -->
</section>
        <footer id="contentinfo" class="body">
                <address id="about" class="vcard body">
                Proudly powered by <a href="http://getpelican.com/">Pelican</a>,
                which takes great advantage of <a href="http://python.org">Python</a>.
                </address><!-- /#about -->
        </footer><!-- /#contentinfo -->
</body>
</html>