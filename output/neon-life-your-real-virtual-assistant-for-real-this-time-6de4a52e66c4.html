<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta name="author" content="Michael Li" />

        <meta name="description" content="NEON.LIFE: Your REAL Virtual Assistant, For Real This Time?
" />
        <meta property="og:type" content="article" />
        <meta name="twitter:card" content="summary">

<meta name="keywords" content="Machine Learning, Artificial Intelligence, Machine Learning, " />

<meta property="og:title" content="NEON.LIFE: Your REAL Virtual Assistant, For Real This Time?  - The anatomy and theory-crafting of NEON.LIFE, Samsung’s new ‘Artificial Human’ "/>
<meta property="og:url" content="https://wayofnumbers.com/neon-life-your-real-virtual-assistant-for-real-this-time-6de4a52e66c4" />
<meta property="og:description" content="NEON.LIFE: Your REAL Virtual Assistant, For Real This Time?" />
<meta property="og:site_name" content="Way of Numbers" />
<meta property="og:article:author" content="Michael Li" />
<meta property="og:article:published_time" content="2020-01-31T15:37:08-06:00" />
<meta name="twitter:title" content="NEON.LIFE: Your REAL Virtual Assistant, For Real This Time?  - The anatomy and theory-crafting of NEON.LIFE, Samsung’s new ‘Artificial Human’ ">
<meta name="twitter:description" content="NEON.LIFE: Your REAL Virtual Assistant, For Real This Time?">
<meta property="og:image" content="/theme/images/apple-touch-icon-152x152.png" />
<meta name="twitter:image" content="/theme/images/apple-touch-icon-152x152.png" >

        <title>NEON.LIFE: Your REAL Virtual Assistant, For Real This Time?  - The anatomy and theory-crafting of NEON.LIFE, Samsung’s new ‘Artificial Human’  · Way of Numbers
</title>
        <link href="//netdna.bootstrapcdn.com/twitter-bootstrap/2.3.2/css/bootstrap-combined.min.css" rel="stylesheet">
        <link rel="stylesheet" type="text/css" href="https://wayofnumbers.com/theme/css/elegant.prod.css" media="screen">
        <link rel="stylesheet" type="text/css" href="https://wayofnumbers.com/theme/css/custom.css" media="screen">

        <link rel="shortcut icon" href="https://wayofnumbers.com/theme/images/favicon.ico" type="image/x-icon" />
        <link rel="icon" href="https://wayofnumbers.com/theme/images/apple-touch-icon-152x152.png" type="image/png" />
        <link rel="apple-touch-icon" href="https://wayofnumbers.com/theme/images/apple-touch-icon.png"  type="image/png" />
        <link rel="apple-touch-icon" sizes="57x57" href="https://wayofnumbers.com/theme/images/apple-touch-icon-57x57.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="72x72" href="https://wayofnumbers.com/theme/images/apple-touch-icon-72x72.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="76x76" href="https://wayofnumbers.com/theme/images/apple-touch-icon-76x76.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="114x114" href="https://wayofnumbers.com/theme/images/apple-touch-icon-114x114.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="120x120" href="https://wayofnumbers.com/theme/images/apple-touch-icon-120x120.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="144x144" href="https://wayofnumbers.com/theme/images/apple-touch-icon-144x144.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="152x152" href="https://wayofnumbers.com/theme/images/apple-touch-icon-152x152.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="152x152" href="https://wayofnumbers.com/theme/images/apple-touch-icon-180x180.png" type="image/png" />
        <link href="https://wayofnumbers.com/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Way of Numbers - Full Atom Feed" />
<script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
     (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
     m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
     })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-148798026-1', 'auto');
    ga('send', 'pageview');
</script>


    </head>
    <body>
        <div id="content">
            <div class="navbar navbar-static-top">
                <div class="navbar-inner">
                    <div class="container-fluid">
                        <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
                            <span class="icon-bar"></span>
                            <span class="icon-bar"></span>
                            <span class="icon-bar"></span>
                        </a>
                        <a class="brand" href="https://wayofnumbers.com/"><span class=site-name>Way of Numbers</span></a>
                        <div class="nav-collapse collapse">
                            <ul class="nav pull-right top-menu">
                                <li >
                                    <a href=
                                       https://wayofnumbers.com
                                    >Home</a>
                                </li>
                                <li ><a href="https://wayofnumbers.com/about-me">About&nbsp;Me</a></li>
                                <li ><a href="https://wayofnumbers.com/categories">Categories</a></li>
                                <li ><a href="https://wayofnumbers.com/tags">Tags</a></li>
                                <li ><a href="https://wayofnumbers.com/archives">Archives</a></li>
                                <li><form class="navbar-search" action="https://wayofnumbers.com/search" onsubmit="return validateForm(this.elements['q'].value);"> <input type="text" class="search-query" placeholder="Search" name="q" id="tipue_search_input"></form></li>
                            </ul>
                        </div>
                    </div>
                </div>
            </div>
            <div class="container-fluid">
                <div class="row-fluid">
                    <div class="span1"></div>
                    <div class="span10">
<article itemscope>
<div class="row-fluid">
    <header class="page-header span10 offset2">
        <h1>
            <a href="https://wayofnumbers.com/neon-life-your-real-virtual-assistant-for-real-this-time-6de4a52e66c4">
                <span class="caps">NEON</span>.<span class="caps">LIFE</span>: Your <span class="caps">REAL</span> Virtual Assistant, For Real This&nbsp;Time?
                <small class="subtitle">
                    The anatomy and theory-crafting of NEON.LIFE, Samsung’s new ‘Artificial Human’
                </small>
            </a>
        </h1>
    </header>
</div>

    <span><p style="text-align:right; color:#aaaaaa; ">&nbsp Estimated read time: 11 min.</p></span>

<div class="row-fluid">
    <div class="span2 table-of-content">
        <nav>
        <h4>Contents</h4>
        <div class="toc">
<ul>
<li><a href="#encounter">Encounter</a></li>
<li><a href="#anatomy">Anatomy</a></li>
<li><a href="#physical-modeling-real-human-video-to-cgi-cross-the-uncanny-valley">Physical modeling: Real-human video to <span class="caps">CGI</span>, cross the uncanny valley</a></li>
<li><a href="#expression-modeling-expressiongesture-projection-to-cgi">Expression modeling: Expression/Gesture projection to <span class="caps">CGI</span></a></li>
<li><a href="#personality-modeling-emotion-to-expression">Personality modeling: Emotion to expression</a></li>
<li><a href="#what-samsung-say-about-neon-is-comprised-of">What Samsung Say About <span class="caps">NEON</span> is comprised of</a></li>
<li><a href="#theorycrafting">Theorycrafting</a></li>
<li><a href="#vision">Vision</a></li>
<li><a href="#conclusion">Conclusion</a></li>
</ul>
</div>
        </nav>
    </div>
    <div class="span8 article-content">
            
            
<p><img alt="" src="https://cdn-images-1.medium.com/max/2500/0*pbfygQlozOYzIXYx.jpg"/></p>
<p>Anyone watched <a href="https://www.imdb.com/title/tt1856101/">Blade Running 2049</a> must remember ‘Joi’, the pretty and sophisticated holographic projection of an artificial human. She speaks to you, helps you with house affairs, tells jokes to you, keeps you accompanied, and some more… just like a real human. She even has her own memory with you and developed character over time. Except, ‘she’ is not human. She is just a super complicated ‘modeling’ of a real human that can speak, act and react like one. Yet still, quite some people secretly wish that they could also have their own ‘Joi’. Well, she might not be as far away as you think. Enter <a href="https://www.neon.life/"><span class="caps">NEON</span></a>, Samsung’s new artificial human.</p>
<p><img alt="Joi from Gfycat" src="https://cdn-images-1.medium.com/max/2000/0*qewd9ME91MB7hJnH.gif"/><em>Joi from <a href="https://gfycat.com/grouchyfaintesok">Gfycat</a></em></p>
<h2 id="encounter">Encounter<a class="headerlink" href="#encounter" title="Permanent link"> </a></h2>
<p>I was at <span class="caps">CES</span> 2020 show last week and walking the floor. There were many new gadgets, new technologies revealed like past years. There were also tons of displays. Small, big, huge, foldable, half-transparent, you name it. Among them, one display grabbed my attention. A life-like human stands within a display looking at me with a warm smile and also talks about something with rich gestures. What is this for? A new remote video service maybe? Intrigued, I came closer and checked. Turns out, none of these very realistic figures are human. They are called NEONs, artificial humans created in a Samsung-backed company called <span class="caps">STAR</span> Labs.</p>
<blockquote>
<p><em><span class="caps">NEON</span>, our first artificial human is here. <span class="caps">NEON</span> is a computationally created virtual being that looks and behaves like a real human, with the ability to show emotions and intelligence. — <span class="caps">STAR</span> Labs</em></p>
</blockquote>
<p><img alt="From NEON.LIFE" src="https://cdn-images-1.medium.com/max/2000/0*4apnXkka8XrrtBaX.jpg"/><em>From <span class="caps">NEON</span>.<span class="caps">LIFE</span></em></p>
<p>The look, expression, gesture are so natural I can’t really tell whether it is pre-recorded video or <span class="caps">CGI</span>. Look closer, it is obviously <span class="caps">CGI</span>, just very very ‘real’. Real in the sense of ‘human-like’ rather than ‘high-resolution’. I dug in deeper and found out that these are <span class="caps">AI</span>-generated <span class="caps">CGI</span> footage based on the pre-recorded real human video, a ‘recreation’ from human actor videos. Very much like what <a href="https://www.imdb.com/name/nm0000116/?ref_=tt_ov_dr">James Cameron</a> first did it in the movie <a href="https://www.imdb.com/title/tt0499549/">Avatar</a>.</p>
<p><img alt="Photo from lyon.onvasortir.com" src="https://cdn-images-1.medium.com/max/2000/0*1a6inxMk4U1I2xVH.jpg"/><em>Photo from <a href="http://lyon.onvasortir.com/pourquoi-j-039ai-pas-mange-mon-pere-3873571.html">lyon.onvasortir.com</a></em></p>
<p>But <span class="caps">NEON</span> didn’t stop there, it pushes things even further. These NEONs can go out-of-script and develop its own ‘personality’. It can originally generate new expressions, gestures, and reactions out of its own unique ‘personality’. These ‘personalities’ can also be trained and adapted to outside stimuli. Now, this is deeper than just mimic the facial expression! How did Samsung achieve this and what does this mean? There aren’t too many details revealed at the show. My inner Data Scientists instantly got turned on, let’s try to figure out (guess) how this is achieved and what impact it could have on our society and industry, shall we?</p>
<h2 id="anatomy">Anatomy<a class="headerlink" href="#anatomy" title="Permanent link"> </a></h2>
<p>So how did they do this? Let’s first look at what it can do. To achieve what they claimed, <span class="caps">NEON</span> need to do several things:</p>
<p><em>Notes: The below parts are just my ‘educated prediction’, thus bearing no truth on how <span class="caps">NEON</span> actually works or created. More details of <span class="caps">NEON</span> has yet to be released by Samsung.</em></p>
<h2 id="physical-modeling-real-human-video-to-cgi-cross-the-uncanny-valley">Physical modeling: Real-human video to <span class="caps">CGI</span>, cross the uncanny valley<a class="headerlink" href="#physical-modeling-real-human-video-to-cgi-cross-the-uncanny-valley" title="Permanent link"> </a></h2>
<p>Given some video datasets of one actor, the model needs to learn how to transfer a video into <span class="caps">CGI</span> footage, the more similar the better. This technology has been well developed with the rise of Avatar and performance capture. Actors get filmed with some color dots grid on their faces to record their facial expressions and transfer those facial expressions grid movements into <span class="caps">CGI</span> character expressions. It’s a rather mature technology. What <span class="caps">NEON</span> did is just a bit different. It might not have the grid as a reference but using deep learning, it’s not too hard for the deep neural networks to find the features they need for the task and it can maybe do better than a simpler model like the facial grid.</p>
<p><center><iframe allowfullscreen="" frameborder="0" height="315" src="https://www.youtube.com/embed/OJ1JzYPjcj0" width="560"></iframe></center></p>
<p><strong>Data Input:</strong> video footages</p>
<p><strong>Data Output:</strong> facial/body grid movements time series data.</p>
<h2 id="expression-modeling-expressiongesture-projection-to-cgi">Expression modeling: Expression/Gesture projection to <span class="caps">CGI</span><a class="headerlink" href="#expression-modeling-expressiongesture-projection-to-cgi" title="Permanent link"> </a></h2>
<p>So from the video footage, we now have a grid movements time series data set, if we can label these data set with different expressions, we should be able to train some kind of autoencoder that can encode the <span class="caps">CGI</span> time-series to expression encoding, then re-generate the same <span class="caps">CGI</span> time-series data. Then we can look at the encoding and figure out what is smile, cry, surprise, angry, etc. This is also a solved problem. You can find one examples below:</p>
<p><center><iframe allowfullscreen="" frameborder="0" height="315" src="https://www.youtube.com/embed/dCKbRCUyop8" width="560"></iframe></center></p>
<p><strong>Data Input:</strong> <span class="caps">CGI</span> Time-series data set</p>
<p><strong>Data Output:</strong> Expression encoding/embedding layers</p>
<h2 id="personality-modeling-emotion-to-expression">Personality modeling: Emotion to expression<a class="headerlink" href="#personality-modeling-emotion-to-expression" title="Permanent link"> </a></h2>
<p>So now we can pretty much control the expression of our <span class="caps">CGI</span> avatar via expression encoding, the next step would be to map emotion to expression. The expression is the externalizing of emotion, but the mapping isn’t always straightforward. You would think people will laugh when they are happy, but humanity is way more complicated than that. Some outgoing people will laugh out loud while an introvert will probably just smirk subtly. What controls the mapping from emotion to expression is personality. Now how do we model personality? This requires a lot of domain knowledge(this is also what Samsung claims the part they are still working on and I think the most challenging. Cause humanity, you know…).</p>
<p><img alt="" src="https://cdn-images-1.medium.com/max/3840/0*4kfOSTR1OQfyoXf1"/></p>
<p><strong>Data Input:</strong> Emotion Labeling (multiple labels since one expression could have multiple emotions behinds it)</p>
<p><strong>Data Output:</strong> Expression encoding/embeddings representing the emotion for given avatar</p>
<h2 id="what-samsung-say-about-neon-is-comprised-of">What Samsung Say About <span class="caps">NEON</span> is comprised of<a class="headerlink" href="#what-samsung-say-about-neon-is-comprised-of" title="Permanent link"> </a></h2>
<p>From the presentation we know that <span class="caps">NEON</span> is actually comprised of three parts:</p>
<ul>
<li>
<p>Behavioral Neural Network (<strong>Expression Model?</strong>)</p>
</li>
<li>
<p>Evolutionary Generative Intelligence (<strong>Personality Model?</strong>)</p>
</li>
<li>
<p>Computational reality (<strong>Physical Model?</strong>)</p>
</li>
</ul>
<p>Are my above assumptions true? We’ll know more in the near future, but please leave some responses if you have different ideas!</p>
<h2 id="theorycrafting">Theorycrafting<a class="headerlink" href="#theorycrafting" title="Permanent link"> </a></h2>
<p>So how do these parts fit together and form a <span class="caps">NEON</span>? It could work as a pipeline:</p>
<p>First using the video footage of one human actor to train a neural network that can generate <span class="caps">CGI</span> grid time-series data. This will give the ability to control the <span class="caps">CGI</span> avatar to be as human as possible. The neural network will inevitably learn how human gestures and human facial expressions patterns, this lays the ground for further abstraction.</p>
<p>With the <span class="caps">CGI</span> grid movement time-series data, we can train an autoencoder that can do some kind of dimensional reduction, create some middle layer encoder then regenerate the <span class="caps">CGI</span>. Once the regenerated <span class="caps">CGI</span> is similar enough to the original time-series data, we’ll get an even more abstract layer (expression encoder) of the video, expressions, and gestures. Once we have the encoder or embedding, we can play around and see which combination of weights could generate certain expressions, e.g. smile, angry, surprised, etc. We can then use these weights ( might need to do some <span class="caps">PCA</span> to make it more manageable) to control the expression of our avatar, make it smile, cry, etc.</p>
<p>To this point, the avatar’s expression could be largely controlled manually, but it’s not there yet. What’s needed is to have the avatar itself originally ‘generate’ expressions on its own, react to outside stimuli. This is where the personality model comes into play. Using the domain knowledge of phycology, emotional science, many different personality features can be developed their relationship to expression and outside stimuli can be modeled. If we use certain celebrity (say Bruce Lee) as an example, by labeling his behavior (expression) and outside stimulus (sentiment of words, gestures, etc.), we can develop a ‘personality’ model that reflects what the celebrity will react to the different sentiment with different expressions. Then we use this personality model to control the <span class="caps">NEON</span>’s reaction expression according to outside sentiment ( output from a sentiment classifier neural network maybe).</p>
<p>In the <span class="caps">NEON</span> official videos, there is a real-time visualization of <span class="caps">NEON</span>’s ‘emotion activation’ status, indicating how <span class="caps">NEON</span> react to outside stimulus. Pretty cool.</p>
<p><img alt="NEON’s emotional map in real-time, the light bulb is where her current state is and the words are different emotional states." src="https://cdn-images-1.medium.com/max/2000/1*qdaUUj1a0AmtNuoV_Erg4g.png"/><em><span class="caps">NEON</span>’s emotional map in real-time, the light bulb is where her current state is and the words are different emotional states.</em></p>
<p>Beyond that, <span class="caps">NEON</span> can learn domain knowledge and provide more value. Samsung claims that they have another cloud <span class="caps">AI</span> platform called Spectra for that and 3rd party developers will be able to develop those ‘skills’ for <span class="caps">NEON</span>. Thinking Siri with a pretty face, charming voice, and can teach you Kung Fu. 😜</p>
<p>I have to say, even though the demo they showed on <span class="caps">CES</span> 2020 is far from perfect, <span class="caps">NEON</span> is groundbreaking. The team is thinking quite big and laid down a good foundation and framework. The computational hunger application at this moment may mean <span class="caps">NEON</span> can’t live on the edge, but with the fast development of 5G and better cloud platform, I believe the future potential for <span class="caps">NEON</span> is huge.</p>
<h2 id="vision">Vision<a class="headerlink" href="#vision" title="Permanent link"> </a></h2>
<p><img alt="Which NEON do you want? Yoga tutor? Magician? Business assistant? Personal Photographer?" src="https://cdn-images-1.medium.com/max/3840/0*Bxam4hzol0C0TINZ"/><em>Which <span class="caps">NEON</span> do you want? Yoga tutor? Magician? Business assistant? Personal Photographer?</em></p>
<p>This is the part of the article where I’m allowed to go wild. Let’s see what a <span class="caps">NEON</span> can do:</p>
<ul>
<li><strong>Perfect <span class="caps">NPC</span> in gaming. </strong>In <span class="caps">RPG</span> games, NPCs are usually pretty dumb. Their facial expression and reaction are so fake. <span class="caps">NEON</span> can change that and give gamers a very real experience.</li>
</ul>
<p><center><iframe allowfullscreen="" frameborder="0" height="315" src="https://www.youtube.com/embed/Vzxv-MKtXZc" width="560"></iframe></center></p>
<ul>
<li>
<p><strong>Recreate your passed away loved ones or friends. </strong>If you’ve read <a href="https://www.wired.com/story/a-sons-race-to-give-his-dying-father-artificial-immortality/">this article</a> about a boy tried to create a chatbot speak exactly like his dad and kept him accompany after the old man passed away, you know what I’m talking about. <span class="caps">NEON</span> can go even further on this, not only the chatbot generate texts like his father, it can look like, sounds like, act as his father and even with similar ‘personality’. (maybe a bit creepy but who am I to judge…)</p>
</li>
<li>
<p><strong>All kinds of service assistants. </strong>Like Yoga tutor, financial planner, or just keep you company with a good personality.</p>
</li>
<li>
<p><strong>Guidance/Helper for disabled people.</strong></p>
</li>
<li>
<p><strong>Celebrity ‘copy’ for fans to adore</strong></p>
</li>
<li>
<p><strong>Autism therapist</strong></p>
</li>
</ul>
<p>The list can go on and on, but you get the idea. With it being an open platform, it could very well be the next big thing in tech.</p>
<h2 id="conclusion">Conclusion<a class="headerlink" href="#conclusion" title="Permanent link"> </a></h2>
<p>Usually, <span class="caps">CES</span> is about engineering innovations rather than science breakthrough, but I think <span class="caps">NEON</span> is somewhat sit in the middle. The team is still very young and I’m so excited to see what they can do in the near future and learn about their approaches. It is really a hidden gem that I felt compelled to introduce to my readers. What do you think an artificial human can do? Cannot do? Or should never do?</p>
<p>For more details on what’s shown on the stage of <span class="caps">CES</span>, you can check out this detailed video:</p>
<p><center><iframe allowfullscreen="" frameborder="0" height="315" src="https://www.youtube.com/embed/lXZBmMGD7pI" width="560"></iframe></center></p>


             
 
            
                <hr />
    <div class="author_blurb">
        <a href="https://medium.com/@lymenlee" target="_blank" rel="nofollow noopener noreferrer">
            <img src=/images/michael.png alt="Michael Li Avatar" title="Michael Li">
            <span class="author_name">Michael Li</span>
        </a>
        is the creator and lead developer of this site.
    </div>

            






            <hr/>
            <aside>
            <nav>
            <ul class="articles-timeline">
                <li class="previous-article">« <a href="https://wayofnumbers.com/build-a-fashion-mnist-cnn-pytorch-style-efb297e22582" title="Previous: Let’s Build a Fashion-MNIST CNN, PyTorch Style - A Line-by-line guide on how to structure a PyTorch ML project from scratch using Google Colab and TensorBoard">Let’s Build a Fashion-MNIST CNN, PyTorch Style <small class="subtitle">A Line-by-line guide on how to structure a PyTorch ML project from scratch using Google Colab and TensorBoard</small></a></li>
                <li class="next-article"><a href="https://wayofnumbers.com/how-to-port-your-medium-articles-to-personal-blog-with-a-simple-bash-script-4422aa7b9be" title="Next: How to Port Your Medium Articles to Personal Blog with a Simple Bash Script - ‘Quick and Dirty’ Blogging Automation">How to Port Your Medium Articles to Personal Blog with a Simple Bash Script <small class="subtitle">‘Quick and Dirty’ Blogging Automation</small></a> »</li>
            </ul>
            </nav>
            </aside>
        </div>
        <section id="article-sidebar" class="span2">
            <h4>Published</h4>
            <time itemprop="dateCreated" datetime="2020-01-31T15:37:08-06:00">Jan 31, 2020</time>
            <h4>Category</h4>
            <a class="category-link" href="https://wayofnumbers.com/categories#machine-learning-ref">Machine Learning</a>
            <h4>Tags</h4>
            <ul class="list-of-tags tags-in-article">
                <li><a href="https://wayofnumbers.com/tags#artificial-intelligence-ref">Artificial Intelligence
                    <span>9</span>
</a></li>
                <li><a href="https://wayofnumbers.com/tags#machine-learning-ref">Machine Learning
                    <span>22</span>
</a></li>
            </ul>
<h4>Stay in Touch</h4>
<div id="sidebar-social-link">
    <a href="https://twitter.com/lymenlee" title="My Twitter" target="_blank" rel="nofollow noopener noreferrer">
        <svg xmlns="http://www.w3.org/2000/svg" aria-label="Twitter" role="img" viewBox="0 0 512 512"><rect width="512" height="512" rx="15%" fill="#1da1f3"/><path fill="#fff" d="M437 152a72 72 0 0 1-40 12 72 72 0 0 0 32-40 72 72 0 0 1-45 17 72 72 0 0 0-122 65 200 200 0 0 1-145-74 72 72 0 0 0 22 94 72 72 0 0 1-32-7 72 72 0 0 0 56 69 72 72 0 0 1-32 1 72 72 0 0 0 67 50 200 200 0 0 1-105 29 200 200 0 0 0 309-179 200 200 0 0 0 35-37"/></svg>
    </a>
    <a href="https://github.com/wayofnumbers" title="GitHub" target="_blank" rel="nofollow noopener noreferrer">
        <svg xmlns="http://www.w3.org/2000/svg" aria-label="GitHub" role="img" viewBox="0 0 512 512"><rect width="512" height="512" rx="15%" fill="#1B1817"/><path fill="#fff" d="M335 499c14 0 12 17 12 17H165s-2-17 12-17c13 0 16-6 16-12l-1-50c-71 16-86-28-86-28-12-30-28-37-28-37-24-16 1-16 1-16 26 2 40 26 40 26 22 39 59 28 74 22 2-17 9-28 16-35-57-6-116-28-116-126 0-28 10-51 26-69-3-6-11-32 3-67 0 0 21-7 70 26 42-12 86-12 128 0 49-33 70-26 70-26 14 35 6 61 3 67 16 18 26 41 26 69 0 98-60 120-117 126 10 8 18 24 18 48l-1 70c0 6 3 12 16 12z"/></svg>
    </a>
    <a href="www.linkedin.com/in/michael-li-dfw" title="LinkedIn" target="_blank" rel="nofollow noopener noreferrer">
        <svg xmlns="http://www.w3.org/2000/svg" aria-label="LinkedIn" role="img" viewBox="0 0 512 512" fill="#fff"><rect width="512" height="512" rx="15%" fill="#0077b5"/><circle cx="142" cy="138" r="37"/><path stroke="#fff" stroke-width="66" d="M244 194v198M142 194v198"/><path d="M276 282c0-20 13-40 36-40 24 0 33 18 33 45v105h66V279c0-61-32-89-76-89-34 0-51 19-59 32"/></svg>
    </a>
</div>
            



<!-- Begin MailChimp Signup Form -->
<div id="mc-embed-signup">
<form action="https://github.us17.list-manage.com/subscribe/post?u=c212184cc0965bdf1658f69f0&amp;id=5677a7b75e" method="post" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" class="validate" target="_blank" novalidate>
<h4>Get Monthly Updates</h4>
<input type="email" value="" name="EMAIL" class="email" id="mce-EMAIL" placeholder="Enter your email..." required>
<div class="clear"><input type="submit" value="Send me Free Updates" name="subscribe" id="mc-embedded-subscribe" class="button"></div>
</form>
</div>
<!--End mc_embed_signup-->




            



        </section>
</div>
</article>
                    </div>
                    <div class="span1"></div>
                </div>
            </div>
        </div>
<footer>
    <div>
        Content licensed under <a rel="license nofollow noopener noreferrer"
    href="http://creativecommons.org/licenses/by/4.0/" target="_blank">
    Creative Commons Attribution 4.0 International License</a>.
    </div>

    <div>
        <span class="site-name">Way of Numbers</span> - Data science for the rest of us.
    </div>



    <div id="fpowered">
        Powered by: <a href="http://getpelican.com/" title="Pelican Home Page" target="_blank" rel="nofollow noopener noreferrer">Pelican</a>
        Theme: <a href="https://elegant.oncrashreboot.com/" title="Theme Elegant Home Page" target="_blank" rel="nofollow noopener noreferrer">Elegant</a>
        Hosted on:
        <a href=https://www.netlify.com/ target="_blank" rel="nofollow noopener noreferrer">
            Netlify
        </a>
    </div>
</footer>            <script src="//code.jquery.com/jquery.min.js"></script>
        <script src="//netdna.bootstrapcdn.com/twitter-bootstrap/2.3.2/js/bootstrap.min.js"></script>
        <script>
            function validateForm(query)
            {
                return (query.length > 0);
            }
        </script>

    <script>
    (function () {
        if (window.location.hash.match(/^#comment-\d+$/)) {
            $('#comment_thread').collapse('show');
        }
    })();
    window.onhashchange=function(){
        if (window.location.hash.match(/^#comment-\d+$/))
            window.location.reload(true);
    }
    $('#comment_thread').on('shown', function () {
        var link = document.getElementById('comment-accordion-toggle');
        var old_innerHTML = link.innerHTML;
        $(link).fadeOut(200, function() {
            $(this).text('Click here to hide comments').fadeIn(200);
        });
        $('#comment_thread').on('hidden', function () {
            $(link).fadeOut(200, function() {
                $(this).text(old_innerHTML).fadeIn(200);
            });
        })
    })
</script>

    </body>
    <!-- Theme: Elegant built for Pelican
        License : MIT -->
</html>